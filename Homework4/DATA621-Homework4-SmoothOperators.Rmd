---
title: "DATA621-Homework4-SmoothOperators"
author: "Rob Hodde, Matt Farris, Jeffrey Burmood, Bin Lin"
date: "4/17/2017"
output: pdf_document
---  
  
###Problem Description

The objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. 
  
Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.  

Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. For the binary logistic regression model, will use a metric such as log likelihood, AIC, ROC curve, etc.? Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.  
  
Approach Steps:
  
1) Build a logistic regression model based on the TARGET_FLAG response variable.  
  
2) Generate TARGET_FLAG predictions using the logistic regression model.  
  
3) Build a linear regression model based on the non-zero values of the TARGET_AMT response variable.  
  
4) Generate TARGET_AMT predictions using the linear regression model based on the non-zero values of the predicted TARGET_FLAG variable.  
  
  
\begin{center}
{\huge Data Exploration}
\end{center}

---

##Data Exploration


```{r,echo=FALSE,warning=FALSE}
# Load required libraries
library(ggplot2)
library(ROCR)
library(RCurl)
library(knitr)
library(Hmisc)
library(caret)
library(stringr)
library(mice)
library(dplyr)
library(reshape2)
#
# Read in the dataset from github
ins <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework4/data/insurance_training_data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)
ins_eval <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework4/data/insurance-evaluation-data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)
```
  

```{r,echo=FALSE,warning=FALSE}

#DATA CLEANSING

#FUNCTIONS---------------
#convert character values to numeric 
sToNum <- function(a){
  a <- gsub("[$]","", a)
  a <- gsub("[,]","", a)
  a <- as.numeric(a)
  return(a)
}
#convert binary choices to numeric 1,0.   1 being the less likely to be in accident
sToBLN <- function(a){
  a <- gsub("No","0", a)
  a <- gsub("Yes", "1", a)
  a <- gsub("no","0", a)
  a <- gsub("yes", "1", a)
  a <- gsub("M","0", a)
  a <- gsub("F", "1", a)
  a <- gsub("Commercial","0", a)
  a <- gsub("Private", "1", a)
  a <- gsub("Rural", "0", a)
  a <- gsub("Urban","1", a)
  a <- as.numeric(a)
  return(a)
}

#use this when a "YES" answer is a bad thing, like prior accidents or license revoked
sToBLN_Reverse <- function(a){
  a <- gsub("No","1", a)
  a <- gsub("Yes", "0", a)
  a <- gsub("no","1", a)
  a <- gsub("yes", "0", a)
  a <- as.numeric(a)
  return(a)
}

#MAKE A COPY OF THE IMPORTED TRAINING DATA FRAME
insc <- ins 

#clean up unfriendly values
insc$MSTATUS    <- str_replace(insc$MSTATUS, pattern = "z_No", replacement = "No")
insc$SEX        <- str_replace(insc$SEX, pattern = "z_F", replacement = "F")
insc$EDUCATION  <- str_replace(insc$EDUCATION, pattern = "z_High School", replacement = "Secondary")
insc$EDUCATION  <- str_replace(insc$EDUCATION, pattern = "<High School", replacement = "Primary")
insc$JOB        <- str_replace(insc$JOB, pattern = "z_Blue Collar", replacement = "Tradesperson")
insc$CAR_TYPE   <- str_replace(insc$CAR_TYPE, pattern = "z_SUV", replacement = "SUV")
insc$URBANICITY <- str_replace(insc$URBANICITY, pattern = "Highly Urban/ Urban", replacement = "Urban")
insc$URBANICITY <- str_replace(insc$URBANICITY, pattern = "z_Highly Rural/ Rural", replacement = "Rural")

insc$CAR_AGE[insc$CAR_AGE < 0 ] <- 0  #If car age is below zero, set to zero
insc$TARGET_AMT <- round(insc$TARGET_AMT,digits = 0) #Round Target Claim to nearest dollar

#Convert data that is errantly classed as character to numeric: INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM
insc$TARGET_FLAG <- sToNum(insc$TARGET_FLAG)
insc$INCOME      <- sToNum(insc$INCOME)
insc$HOME_VAL    <- sToNum(insc$HOME_VAL)
insc$BLUEBOOK    <- sToNum(insc$BLUEBOOK)
insc$OLDCLAIM    <- sToNum(insc$OLDCLAIM)

#create numeric representations of the "binary choice" variables so they can be support multiple imputation (filling in NA values)
insc$blnPARENT1     <- sToBLN(insc$PARENT1)
insc$blnMSTATUS     <- sToBLN(insc$MSTATUS)
insc$blnSEX         <- sToBLN(insc$SEX)
insc$blnCAR_USE     <- sToBLN(insc$CAR_USE)
insc$blnNOT_RED_CAR <- sToBLN_Reverse(insc$RED_CAR) #Since having a red car increases likelihood of accident, use reverse boolean
insc$blnNOT_REVOKED <- sToBLN_Reverse(insc$REVOKED)
insc$blnURBANICITY  <- sToBLN(insc$URBANICITY)

#CREATE numeric representations of the categorical variables (to support multiple imputation) 
#Education: 1-Primary, 2-Secondary, 3-Bachelors, 4-Masters, 5-PhD
educ <- data.frame(EDUCATION = c("Primary","Secondary","Bachelors","Masters","PhD"), intEDUCATION = c(1,2,3,4,5))
insc$intEDUCATION <- educ$intEDUCATION[match(insc$EDUCATION,educ$EDUCATION)]
#Job - 1 is lowest income, 8 is highest:
job <- data.frame(JOB = c("Student","Home Maker","Clerical","Tradesperson","Professional","Manager","Lawyer","Doctor"), intJOB = c(1,2,3,4,5,6,7,8))
insc$intJOB <- job$intJOB[match(insc$JOB, job$JOB)]
#Vehicle Type  by avg blue book: 1-Sports Car has lowest blue book, Panel Truck has highest
typ <- data.frame(CAR_TYPE = c("Sports Car","SUV","Pickup","Minivan","Van","Panel Truck"), intCAR_TYPE = c(1,2,3,4,5,6)) 
insc$intCAR_TYPE <- typ$intCAR_TYPE[match(insc$CAR_TYPE, typ$CAR_TYPE)]

summary(insc) #Show the new attributes and corrections

#mice PACKAGE
#uses Predictive Mean Matching. 
mice.i <- mice(insc, m = 3, print=F)
mice.ic <- complete(mice.i,1)

insi <- subset(mice.ic,select = c("INDEX", "TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

head(insi)
summary(insi)  #  <---- USE insi DATA FRAME for modeling with imputed data  ***
#conclusion: the imputed values are above zero.  wish I could see methods, R-Squares 

#now we must make a dataframe with numeric-only fields
insi <- subset(mice.ic,select = c("INDEX", "TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

head(insi)
insi2 <- insi
```



Below is a summary of each predictor variable's basic statistics, followed by boxplots which illustrate the spread and outliers for each variable.    


```{r,echo=FALSE,warning=FALSE}
# First, remove the INDEX variable column since it's not used
insi <- insi[2:length(insi)]
# Next, get a general look at the data
# Let's start by exploring the type of each variable
types <- sapply(1:length(insi),function(x) typeof(insi[,x]))
types.df <- data.frame(VAR=names(insi),TYPE=types)
kable(types.df)

# Now generate some summary statistics
kable(summary(insi[1:6]))
kable(summary(insi[7:12]))
kable(summary(insi[13:18]))
kable(summary(insi[19:length(insi)]))

# Look over the variables checking for outliers/influential points, correlation between variables, etc. using box plots.
#
# set the plot-page configuration

```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$KIDSDRIV, main="KIDSDRIV")
boxplot(insi$AGE, main="AGE")
boxplot(insi$HOMEKIDS, main="HOMEKIDS")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$YOJ, main="YOJ")
boxplot(insi$HOME_VAL, main="HOME_VAL")
boxplot(insi$intEDUCATION, main="EDUCATION")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$intJOB, main="JOB")
boxplot(insi$INCOME, main="INCOME")
boxplot(insi$TRAVTIME, main="TRAVTIME")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$TIF, main="TIF")
boxplot(insi$intCAR_TYPE, main="CAR_TYPE")
boxplot(insi$BLUEBOOK, main="BLUEBOOK")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$OLDCLAIM, main="OLDCLAIM")
boxplot(insi$CLM_FREQ, main="CLM_FREQ")
boxplot(insi$MVR_PTS, main="MVR_PTS")
```

```{r,echo=FALSE,warning=FALSE,fig.height = 3}
#par(mfcol=c(1,3))
boxplot(insi$CAR_AGE, main="CAR_AGE")
```
   
```{r,echo=FALSE,warning=FALSE}
# Lastly, build a correlation table to identify any relationships between the predictor variables
# get a table of only predictor variables
insi.pred <- subset(insi,select=-c(TARGET_FLAG,TARGET_AMT))

cor.table <- cor(insi.pred) # build a table of inter-variable correlation values
kable(cor.table[,1:8])
kable(cor.table[,9:16])
```
  
Here are the results from an analysis of the predictor variable correlations:  
  
The are no strong correlations (>70%) between predictor variables, not enough to allow consideration of removing a variable from the model based on a high correlation with another variable. There is some moderate correlation (30-50%) between some variable highlighting obvious relationships such as HOMEKIDS-KIDSDRIV, HOME_VAL-INCOME, EDUCATION-INCOME, JOB-INCOME, CAR_TYPE-BLUEBOOK, CLM_FREQ-OLDCLAIM, and MVR_PTS-CLM_FREQ.  
  
Based on an analysis of the box plots, the following variables have some outliers that may, or may not, exert influence on the regression results: 
    - KIDSDRIV, HOME_VAL, TRAVTIME, MVR_PTS, AGE, INCOME, BLUEBOOK, OLDCLAIM  
  
We'll next look at these variables more closely, starting with their histograms and frequency counts to better understand the nature of their distribution.  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# KIDSDRIV
m <- mean(insi$KIDSDRIV)
s <- sd(insi$KIDSDRIV)
par(mfcol=c(1,3))
hist(insi$KIDSDRIV,prob=TRUE,xlab="KIDSDRIV",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# zn is so skewed, let's look at a frequency count
plot(table(insi$KIDSDRIV))
# let's look at a plot of the values
plot(insi$KIDSDRIV)

```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# HOME_VAL
m <- mean(insi$HOME_VAL)
s <- sd(insi$HOME_VAL)
par(mfcol=c(1,3))
hist(insi$HOME_VAL,prob=TRUE,xlab="HOME_VAL",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
#let's look at a frequency count
plot(table(insi$HOME_VAL))
# let's look at a plot of the values
plot(insi$HOME_VAL)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# MVR_PTS
m <- mean(insi$MVR_PTS)
s <- sd(insi$MVR_PTS)
par(mfcol=c(1,3))
hist(insi$MVR_PTS,prob=TRUE,xlab="MVR_PTS",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
#let's look at a frequency count
plot(table(insi$MVR_PTS))
# let's look at a plot of the values
plot(insi$MVR_PTS)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# TRAVTIME
m <- mean(insi$TRAVTIME)
s <- sd(insi$TRAVTIME)
par(mfcol=c(1,3))
hist(insi$TRAVTIME,prob=TRUE,xlab="TRAVTIME",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# TRAVTIME is so skewed, let's look at a frequency count
plot(table(insi$TRAVTIME))
# let's look at a plot of the values
plot(insi$TRAVTIME)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# AGE
m <- mean(insi$AGE)
s <- sd(insi$AGE)
par(mfcol=c(1,3))
hist(insi$AGE,prob=TRUE,xlab="AGE",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# AGE is so skewed, let's look at a frequency count
plot(table(insi$AGE))
# let's look at a plot of the values
plot(insi$AGE)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# INCOME
m <- mean(insi$INCOME)
s <- sd(insi$INCOME)
par(mfcol=c(1,3))
hist(insi$INCOME,prob=TRUE,xlab="INCOME",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# INCOME is so skewed, let's look at a frequency count
plot(table(insi$INCOME))
# let's look at a plot of the values
plot(insi$INCOME)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# BLUEBOOK
m <- mean(insi$BLUEBOOK)
s <- sd(insi$BLUEBOOK)
par(mfcol=c(1,3))
hist(insi$BLUEBOOK,prob=TRUE,xlab="BLUEBOOK",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# BLUEBOOK is so skewed, let's look at a frequency count
plot(table(insi$BLUEBOOK))
# let's look at a plot of the values
plot(insi$BLUEBOOK)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# OLDCLAIM
m <- mean(insi$OLDCLAIM)
s <- sd(insi$OLDCLAIM)
par(mfcol=c(1,3))
hist(insi$OLDCLAIM,prob=TRUE,xlab="OLDCLAIM",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# OLDCLAIM is so skewed, let's look at a frequency count
plot(table(insi$OLDCLAIM))
# let's look at a plot of the values
plot(insi$OLDCLAIM)
```  
  
  
The analysis of the distributions for these variables show varying degrees of skewness, except for the AGE variable, which shows a fairly normal distribution.  
  
For the logistic regression analysis, we would like to remove as much of the skewness as possible from the candidate predictor variables. A transformation analysis was performed and a log transformation of most of the skewed variables result in a near-normal distribution consequently, for the logistic regression model, we will use the log value of the following variables for the modeling: OLDCLAIM, BLUEBOOK, TRAVTIME, HOME_VAL.  

The last step in the data exploration is to examine the correlation tables, to see if there is any potential correlations that will impact our models. The below table shows the a condense correlation table showing only the highest positive and negative correlations (in this case above 0.5 and below -.5)

  
```{r,echo=FALSE,warning=FALSE}
# build a correlation table to study the variable relationships
cor.table <- cor(insi) # build a table of inter-variable correlation values
cor.table <- melt(cor.table)
cor.table <- subset(cor.table, value != 1)
cor.table <- subset(cor.table, value >= .5 | value <= -.5)
cor.table <- distinct(cor.table,value)
#colnames(cor.table)<-c("Var 1","Var 2", "Correlations")
#kable(cor.table)
```
As you can see, certain variables are correlated, and they are values that we would expect to be correlated. We expect there to be a correlation between income and jobs/home value and educations, as these are well established indicators of success. Higher educated people are more likely to secure higher paying jobs, which we can see in the higher correlations for these values. Furthermore, car type is going to be a clear indicator of the value of the car. Sports cars and suvs cost more than compacts. An interestiong but also intuitive correlations exists bewteen gender and car type and car color. The car type is very interesting, as we almost arbitraily set the type to a numeric value 1 - 6 according to this list "Sports Car","SUV","Pickup","Minivan","Van","Panel Truck" respectively. As the correlation is negative, it shows that as the value goes "up/higher" we see less women in these types of cars. Though this is a binary correlation, and the weight of such can be consider not very conclusive, it is interesting to note. The most likely reason for this offset correlation is because panel truck is listed highest, which we can assume would be a predominately male oriented mode of transportation. 

\begin{center}
{\huge Data Preparation}
\end{center}

--------

##Data Preparation

One means of data preparation we decided to do was to take the log of several of the valiables: 
```{r,echo=FALSE,warning=FALSE}
# perform the log transformation for the selected predictor variables and add to the data frame
# no variables used in the log transform can have a value of 0

OLDCLAIM.median <- mean(insi$OLDCLAIM)
HOME_VAL.median <- median(insi$HOME_VAL)
BLUEBOOK.median <- median(insi$BLUEBOOK)
TRAVTIME.median <- median(insi$TRAVTIME)

insi$OLDCLAIM[insi$OLDCLAIM<=0] <- OLDCLAIM.median
insi$HOME_VAL[insi$HOME_VAL<=0] <- HOME_VAL.median

insi$logOLDCLAIM <- log(insi$OLDCLAIM)
insi$logBLUEBOOK <- log(insi$BLUEBOOK)
insi$logTRAVTIME <- log(insi$TRAVTIME)
insi$logHOME_VAL <- log(insi$HOME_VAL)
```


Next a brief review of the linear models of all the variables
```{r,echo=FALSE,warning=FALSE}
# quick look at model with all variables

lin_insi <- subset(insi, TARGET_FLAG ==1)

KIDSDRIV_lm <- lm(TARGET_AMT ~ KIDSDRIV,data=lin_insi)
AGE_lm <- lm(TARGET_AMT ~ AGE,data=lin_insi)
HOMEKIDS_lm <- lm(TARGET_AMT ~ HOMEKIDS,data=lin_insi)
YOJ_lm <- lm(TARGET_AMT ~ YOJ,data=lin_insi)
INCOME_lm <- lm(TARGET_AMT ~ INCOME,data=lin_insi)
TRAVTIME_lm <- lm(TARGET_AMT ~ TRAVTIME,data=lin_insi)
BLUEBOOK_lm <- lm(TARGET_AMT ~ BLUEBOOK,data=lin_insi)
TIF_lm <- lm(TARGET_AMT ~ TIF,data=lin_insi)
OLDCLAIM_lm <- lm(TARGET_AMT ~ OLDCLAIM,data=lin_insi)
CLM_FREQ_lm <- lm(TARGET_AMT ~ CLM_FREQ,data=lin_insi)
MVR_PTS_lm <- lm(TARGET_AMT ~ MVR_PTS,data=lin_insi)
CAR_AGE_lm <- lm(TARGET_AMT ~ CAR_AGE,data=lin_insi)
blnPARENT1_lm <- lm(TARGET_AMT ~ blnPARENT1,data=lin_insi)
blnMSTATUS_lm <- lm(TARGET_AMT ~ blnMSTATUS,data=lin_insi)
blnSEX_lm <- lm(TARGET_AMT ~ blnSEX,data=lin_insi)
blnCAR_USE_lm <- lm(TARGET_AMT ~ blnCAR_USE,data=lin_insi)
blnNOT_RED_CAR_lm <- lm(TARGET_AMT ~ blnNOT_RED_CAR,data=lin_insi)
blnNOT_REVOKED_lm <- lm(TARGET_AMT ~ blnNOT_REVOKED,data=lin_insi)
blnURBANICITY_lm <- lm(TARGET_AMT ~ blnURBANICITY,data=lin_insi)
intEDUCATION_lm <- lm(TARGET_AMT ~ intEDUCATION,data=lin_insi)
intJOB_lm <- lm(TARGET_AMT ~ intJOB,data=lin_insi)
intCAR_TYPE_lm <- lm(TARGET_AMT ~ intCAR_TYPE,data=lin_insi)

models <- list(KIDSDRIV_lm, AGE_lm,HOMEKIDS_lm,YOJ_lm,INCOME_lm,
              TRAVTIME_lm,BLUEBOOK_lm,TIF_lm,OLDCLAIM_lm,CLM_FREQ_lm,
              MVR_PTS_lm,CAR_AGE_lm,blnPARENT1_lm,blnMSTATUS_lm,
              blnSEX_lm,blnCAR_USE_lm,blnNOT_RED_CAR_lm,blnNOT_REVOKED_lm,
              blnURBANICITY_lm,intEDUCATION_lm,intJOB_lm,intCAR_TYPE_lm)
```


```{r,echo=FALSE,warning=FALSE}
#Creation of lists 
var <- c()
p_val <- c()
r2_val <- c()

#For loop to run variables over univarate glms
for(i in models){
  var <- c(var,variable.names(i)[2])
  p_val <- c(p_val,summary(i)$coef[2, "Pr(>|t|)"])
  r2_val <- c(r2_val,summary(i)$adj.r.squared)
}

data.frame(var,p_val,r2_val)
```

\begin{center}
{\huge Build Models}
\end{center}


##Build Models


One method of developing multiple regression models is to take a stepwise approach.  To accomplish this, we combine our knowledge from the data exploration above with logistic regression. Univariate Logistic Regression is a useful method to understand how each predictor variable interacts individually with the target (response) variable. Looking at various statistics, we determine which variable may impact our target the most.
  
###Linear Regression Models  
  
```{r,echo=FALSE,warning=FALSE}
lin_model <- subset(insi, TARGET_FLAG == 1)
Var_lm <- c("TARGET_AMT","TARGET_FLAG","INCOME","TRAVTIME","BLUEBOOK","OLDCLAIM", "YOJ","intEDUCATION","intJOB","CAR_AGE")

log_var <- c("TARGET_AMT","INCOME","BLUEBOOK","OLDCLAIM")

log_lin_model <- lin_model[Var_lm]
log_lin_model[log_var] <- log(log_lin_model[log_var])
log_lin_model <- do.call(data.frame,lapply(log_lin_model, function(x) replace(x, is.infinite(x),0)))
```

```{r,echo=FALSE,warning=FALSE}
model_ols <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                      OLDCLAIM + YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data =lin_model)
summary(model_ols)

plot(fitted(model_ols), residuals(model_ols))

wts <- 1/fitted(lm(abs(residuals(model_ols)) ~ fitted(model_ols)))^2

length(wts)
length(model_ols$residuals)
model_wls <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                      OLDCLAIM + YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data = lin_model, weights = wts)
summary(model_wls)

plot(fitted(model_wls), rstandard(model_wls))
```



```{r,echo=FALSE,warning=FALSE}
log_model_ols <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                      OLDCLAIM + YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data =log_lin_model)
summary(log_model_ols)

plot(fitted(log_model_ols), residuals(log_model_ols))

wts <- 1/fitted(lm(abs(residuals(log_model_ols)) ~ fitted(log_model_ols)))^2

length(wts)
length(log_model_ols$residuals)
log_model_wls <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                      OLDCLAIM + YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data = log_lin_model, weights = wts)
summary(log_model_wls)


Parameters <- c("Accuracy", "Classification Error Rate", "Precision", "Sensitivity", "Specificity", "F1 Score")
plot(fitted(log_model_wls), rstandard(log_model_wls))
```

```{r,echo=FALSE,warning=FALSE}
backward <- step(model_wls)
summary(backward)
```


### Logistic Regression Model 

As a baseline, we start with a multiple logistic regression model that includes every predictor variable:

In this model-and in all the models- we set aside 20% of the training data and use 80% to train the model we then use the model to predict the outcome of the remaining 20% of the data.  The model yields an Area Under Curve of .95, meaning it chose correctly 95% of the time.
  
  
```{r,echo=FALSE,warning=FALSE}
# build the data frame to be used for logistic regression modeling
insi.log <- insi[,!(names(insi) %in% c("OLDCLAIM","TRAVTIME","BLUEBOOK","HOME_VAL"))]

## 80% of the sample size
set.seed(121)
smp_size <- floor(0.80 * nrow(insi.log))

## set the seed to make your partition reproductible
train_ind <- sample(seq_len(nrow(insi.log)), size = smp_size)

train <- insi.log[train_ind, ]
test <- insi.log[-train_ind, ]
```
  
In this scenario we attempt to create the simplest model possible by using only one variable - the one that provides the highest overall AUC (performance) by itself.  We calculate AUC for each variable separately and then select the highest result.  
  
```{r,echo=FALSE,warning=FALSE}
# build single variable models for analysis

KIDSDRIV_model<- glm(TARGET_FLAG ~ KIDSDRIV,family=binomial(link='logit'),data=train)           
HOMEKIDS_model<- glm(TARGET_FLAG ~ HOMEKIDS,family=binomial(link='logit'),data=train)       
AGE_model<- glm(TARGET_FLAG ~ AGE,family=binomial(link='logit'),data=train)        
blnPARENT1_model<- glm(TARGET_FLAG ~ blnPARENT1,family=binomial(link='logit'),data=train)        
logHOME_VAL_model<- glm(TARGET_FLAG ~ logHOME_VAL,family=binomial(link='logit'),data=train)          
logBLUEBOOK_model<- glm(TARGET_FLAG ~ logBLUEBOOK,family=binomial(link='logit'),data=train)          
INCOME_model<- glm(TARGET_FLAG ~ INCOME,family=binomial(link='logit'),data=train)            
CAR_AGE_model<- glm(TARGET_FLAG ~ CAR_AGE,family=binomial(link='logit'),data=train)           
intEDUCATION_model<- glm(TARGET_FLAG ~ intEDUCATION,family=binomial(link='logit'),data=train)            
intJOB_model<- glm(TARGET_FLAG ~ intJOB,family=binomial(link='logit'),data=train)      
blnMSTATUS_model<- glm(TARGET_FLAG ~ blnMSTATUS,family=binomial(link='logit'),data=train)       
intCAR_TYPE_model<- glm(TARGET_FLAG ~ intCAR_TYPE,family=binomial(link='logit'),data=train)       
logOLDCLAIM_model<- glm(TARGET_FLAG ~ logOLDCLAIM,family=binomial(link='logit'),data=train)       
CLM_FREQ_model<- glm(TARGET_FLAG ~ CLM_FREQ,family=binomial(link='logit'),data=train)       
TIF_model<- glm(TARGET_FLAG ~ TIF,family=binomial(link='logit'),data=train)       
MVR_PTS_model<- glm(TARGET_FLAG ~ MVR_PTS,family=binomial(link='logit'),data=train)
blnSEX_model<- glm(TARGET_FLAG ~ blnSEX,family=binomial(link='logit'),data=train)
blnCAR_USE_model<- glm(TARGET_FLAG ~ blnCAR_USE,family=binomial(link='logit'),data=train)
blnNOT_RED_CAR_model<- glm(TARGET_FLAG ~ blnNOT_RED_CAR,family=binomial(link='logit'),data=train)
blnNOT_REVOKED_model<- glm(TARGET_FLAG ~ blnNOT_REVOKED,family=binomial(link='logit'),data=train)
blnURBANICITY_model<- glm(TARGET_FLAG ~ blnURBANICITY,family=binomial(link='logit'),data=train)
logTRAVTIME_model<- glm(TARGET_FLAG ~ logTRAVTIME,family=binomial(link='logit'),data=train)

models <- list(KIDSDRIV_model,HOMEKIDS_model,AGE_model,blnPARENT1_model,logBLUEBOOK_model,INCOME_model,
               CAR_AGE_model,intEDUCATION_model,intJOB_model,blnMSTATUS_model,intCAR_TYPE_model,
               CLM_FREQ_model,TIF_model,MVR_PTS_model,blnSEX_model,blnCAR_USE_model,blnNOT_RED_CAR_model,
               blnNOT_REVOKED_model,blnURBANICITY_model,logTRAVTIME_model,logOLDCLAIM_model,
               logHOME_VAL_model)

#Creation of lists 
var <- c()
p_val <- c()
aic <- c()
auc <- c()
  
cols <- ncol(insi.log)
#For loop to run variables over univarate glms
for(i in models){
    var <- c(var,variable.names(i)[2])
    aic <- c(aic,i$aic)
    p_val <- c(p_val,summary(i)$coef[2, "Pr(>|z|)"])
    p <- predict(i, newdata=subset(test,select=c(3:cols)), type="response")
    pr <- prediction(p, test$TARGET_FLAG)
    prf <- performance(pr, measure = "tpr", x.measure = "fpr")
    auc_perf<- performance(pr, measure = "auc")
    auc_val <- auc_perf@y.values[[1]]
    auc <- c(auc,auc_val)
 }
 
# build a table of the model analysis results
kable(data.frame(var,p_val,aic,auc))
```


#### MODEL 1 - Backward regression starting with all variables
```{r,echo=FALSE,warning=FALSE}
# Simple backward regression
model_3 <- glm(TARGET_FLAG ~ .-TARGET_AMT,family=binomial(link='logit'),data=train)
backward <- step(model_3)
summary(backward)

p <- predict(backward, newdata=subset(test,select=c(3:cols)), type="response")
pr <- prediction(p, test$TARGET_FLAG)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
abline(a = 0, b = 1)

#
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc


m3_prediction <- ifelse(p > 0.5, 1, 0)
confusion_m3 <- confusionMatrix(data = m3_prediction, reference = test[,1])
confusion_m3$table
```

The model reduces to nine variables and yields a nice low residual deviance of 133.9, compared to a null deviance of 515.3.  This roughly means that the model eliminates about 80% of the error compared to choosing at random.  The AUC is .947 which is roughly the same as the full model using all variables.  



Below is table illustrating the various fitness parameters that describe the effectiveness of the models.  All the models are good - from a practical perspective, there is no difference between them.  

```{r,echo=FALSE,warning=FALSE}
# function to calculate the F1 score
f1 <- function(s,p) {
  (2*p*s)/(p+s)
}

Parameters <- c("Accuracy", "Classification Error Rate", "Precision", "Sensitivity", "Specificity", "F1 Score")

Model3 <- c(confusion_m3$overall["Accuracy"], 1 - confusion_m3$overall["Accuracy"], confusion_m3$byClass["Pos Pred Value"], confusion_m3$byClass["Sensitivity"], confusion_m3$byClass["Specificity"], f1(confusion_m3$byClass["Sensitivity"],confusion_m3$byClass["Pos Pred Value"]))

model_summary <- data.frame(Parameters, Model3)

kable(model_summary)
```

\begin{center}
{\huge Choose Model}
\end{center}

--------

##Choose Model


We would like to pick Backward logistic regression model to make prediction for the evaluation dataset (non-zero value). This model has accuracy rate as high as 80%. In the meantime, the precision and sensitivity are at the level of 82% and 92%, which indicate this model is very good at eliminating false negative and false positive situations. Both AUC and F1 Score are around 80%, which is also telling us that it has high accuracy in terms of predicting the final response variables.


```{r, echo=FALSE, warning=FALSE}
#The following is simply the data preparation step. To get evaluation data ready to plug into our model.

eval_copy <- ins_eval
eval_copy$MSTATUS    <- str_replace(eval_copy$MSTATUS, pattern = "z_No", replacement = "No")
eval_copy$SEX        <- str_replace(eval_copy$SEX, pattern = "z_F", replacement = "F")
eval_copy$EDUCATION  <- str_replace(eval_copy$EDUCATION, pattern = "z_High School", replacement = "Secondary")
eval_copy$EDUCATION  <- str_replace(eval_copy$EDUCATION, pattern = "<High School", replacement = "Primary")
eval_copy$JOB        <- str_replace(eval_copy$JOB, pattern = "z_Blue Collar", replacement = "Tradesperson")
eval_copy$CAR_TYPE   <- str_replace(eval_copy$CAR_TYPE, pattern = "z_SUV", replacement = "SUV")
eval_copy$URBANICITY <- str_replace(eval_copy$URBANICITY, pattern = "Highly Urban/ Urban", replacement = "Urban")
eval_copy$URBANICITY <- str_replace(eval_copy$URBANICITY, pattern = "z_Highly Rural/ Rural", replacement = "Rural")

eval_copy$CAR_AGE[eval_copy$CAR_AGE < 0 ] <- 0
eval_copy$TARGET_AMT <- round(eval_copy$TARGET_AMT,digits = 0)

eval_copy$TARGET_FLAG <- sToNum(eval_copy$TARGET_FLAG)
eval_copy$INCOME      <- sToNum(eval_copy$INCOME)
eval_copy$HOME_VAL    <- sToNum(eval_copy$HOME_VAL)
eval_copy$BLUEBOOK    <- sToNum(eval_copy$BLUEBOOK)
eval_copy$OLDCLAIM    <- sToNum(eval_copy$OLDCLAIM)

eval_copy$blnPARENT1     <- sToBLN(eval_copy$PARENT1)
eval_copy$blnMSTATUS     <- sToBLN(eval_copy$MSTATUS)
eval_copy$blnSEX         <- sToBLN(eval_copy$SEX)
eval_copy$blnCAR_USE     <- sToBLN(eval_copy$CAR_USE)
eval_copy$blnNOT_RED_CAR <- sToBLN_Reverse(eval_copy$RED_CAR)
eval_copy$blnNOT_REVOKED <- sToBLN_Reverse(eval_copy$REVOKED)
eval_copy$blnURBANICITY  <- sToBLN(eval_copy$URBANICITY)

educ <- data.frame(EDUCATION = c("Primary","Secondary","Bachelors","Masters","PhD"), intEDUCATION = c(1,2,3,4,5))
eval_copy$intEDUCATION <- educ$intEDUCATION[match(eval_copy$EDUCATION,educ$EDUCATION)]
job <- data.frame(JOB = c("Student","Home Maker", "Clerical", "Tradesperson", "Professional", "Manager", "Lawyer", "Doctor"), intJOB = c(1,2,3,4,5,6,7,8))
eval_copy$intJOB <- job$intJOB[match(eval_copy$JOB, job$JOB)]
typ <- data.frame(CAR_TYPE = c("Sports Car","SUV","Pickup","Minivan","Van","Panel Truck"), intCAR_TYPE = c(1,2,3,4,5,6)) 
eval_copy$intCAR_TYPE <- typ$intCAR_TYPE[match(eval_copy$CAR_TYPE, typ$CAR_TYPE)]

mice.i <- mice(eval_copy, m = 3, print=F)
mice.ic <- complete(mice.i,1)

eval_copy <- subset(mice.ic, select = c("TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

OLDCLAIM.median <- mean(eval_copy$OLDCLAIM)
HOME_VAL.median <- median(eval_copy$HOME_VAL)
BLUEBOOK.median <- median(eval_copy$BLUEBOOK)
TRAVTIME.median <- median(eval_copy$TRAVTIME)

eval_copy$OLDCLAIM[eval_copy$OLDCLAIM <= 0] <- OLDCLAIM.median
eval_copy$HOME_VAL[eval_copy$HOME_VAL <= 0] <- HOME_VAL.median

eval_copy$logOLDCLAIM <- log(eval_copy$OLDCLAIM)
eval_copy$logBLUEBOOK <- log(eval_copy$BLUEBOOK)
eval_copy$logTRAVTIME <- log(eval_copy$TRAVTIME)
eval_copy$logHOME_VAL <- log(eval_copy$HOME_VAL)
```


```{r, echo=FALSE, warning=FALSE}
# Built the model using the train dataset. Then we save the results under TARGET_FLAG in eval dataset.
model_3 <- glm(TARGET_FLAG ~ KIDSDRIV + INCOME + TIF + CLM_FREQ + MVR_PTS + blnPARENT1 + blnMSTATUS + blnCAR_USE + blnNOT_REVOKED + blnURBANICITY + intEDUCATION + intJOB + intCAR_TYPE + logOLDCLAIM + logBLUEBOOK + logTRAVTIME + logHOME_VAL, family = binomial(link='logit'), data = train)
summary(model_3)

p <- predict(model_3, newdata = eval_copy, type = "response")
eval_copy$TARGET_FLAG <- ifelse(p >= .50, 1, 0)
```


For linear regression model, all the models we created only contain one variable, which is BLUEBOOK. According to the following summary statistics, none of the model is performing significantly better than the others. They have very similar p-value, mean squared error, r-squared, and F-statistics. The R-sqaured is very low, even the best model we have is only 1.63%, which might indicate our model lack validity. Or it could also indicate the claim amount of motor vehicle accident tends to be unpredictable.


```{r, echo=FALSE, warning=FALSE}
Parameters <- c("p-value", "Mean Squared Error", "R^2", "F-Statistics")

m1 <- c(summary(model_ols)$coef[4, "Pr(>|t|)"], mean(summary(model_ols)$residuals^2), summary(model_ols)$r.squared, summary(model_ols)$fstatistic[1])
m2 <- c(summary(model_wls)$coef[4, "Pr(>|t|)"], mean(summary(model_wls)$residuals^2), summary(model_wls)$r.squared, summary(model_wls)$fstatistic[1])
m3 <- c(summary(log_model_ols)$coef[4, "Pr(>|t|)"], mean(summary(log_model_ols)$residuals^2), summary(log_model_ols)$r.squared, summary(log_model_ols)$fstatistic[1])
m4 <- c(summary(log_model_wls)$coef[4, "Pr(>|t|)"], mean(summary(log_model_wls)$residuals^2), summary(log_model_wls)$r.squared, summary(log_model_wls)$fstatistic[1])


#I got a out of bound error for m5, I do not know how to fix it. 

#m5 <- c(summary(backward)$coef[4, "Pr(>|t|)"], mean(summary(backward)$residuals^2), summary(backward)$r.squared, summary(backward)$fstatistic[1])

model_summary <- data.frame(Parameters, m1, m2, m3, m4)
model_summary
```


```{r, echo=FALSE, warning=FALSE}
#Build model based on the independent variable BLUEBOOK. Then predict the result for eval

linear_model <- lm(TARGET_AMT ~ BLUEBOOK, data = insi)
summary(linear_model)
eval_copy$TARGET_AMT <- predict(linear_model, newdata = eval_copy, type = "response")
```


```{r, echo=FALSE, warning=FALSE}
kable(head(eval_copy[,c(1:10)], 20))
```

The Smooth Operators of R Fusion Have Struck Again.