---
title: "DATA621-Homework4-SmoothOperators"
author: "Rob Hodde, Matt Farris, Jeffrey Burmood, Bin Lin"
date: "4/17/2017"
output:  pdf_document
  
  
--------  
  
###Problem Description

The objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. 
  
Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.  

Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. For the binary logistic regression model, will use a metric such as log likelihood, AIC, ROC curve, etc.? Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.  
  
Approach Steps:
  
1) Build a logistic regression model based on the TARGET_FLAG response variable.  
  
2) Generate TARGET_FLAG predictions using the logistic regression model.  
  
3) Build a linear regression model based on the non-zero values of the TARGET_AMT response variable.  
  
4) Generate TARGET_AMT predictions using the linear regression model based on the non-zero values of the predicted TARGET_FLAG variable.  
  
  
\begin{center}
{\huge Data Exploration}
\end{center}

--------

##Data Exploration


```{r,echo=FALSE,warning=FALSE}
# Load required libraries
library(ggplot2)
library(ROCR)
library(RCurl)
library(knitr)
library(Hmisc)
library(caret)
library(stringr)
##library(Amelia)
##library(Hmisc)
##library(mi)
library(mice)

#
# Read in the dataset from github
ins <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework4/data/insurance_training_data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)
ins_eval <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework4/data/insurance-evaluation-data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)

```
  

```{r,echo=FALSE,warning=FALSE}

#DATA CLEANSING

#FUNCTIONS---------------
#convert character values to numeric 
sToNum <- function(a){
  a <- gsub("[$]","", a)
  a <- gsub("[,]","", a)
  a <- as.numeric(a)
  return(a)
}
#convert binary choices to numeric 1,0.   1 being the less likely to be in accident
sToBLN <- function(a){
  a <- gsub("No","0", a)
  a <- gsub("Yes", "1", a)
  a <- gsub("no","0", a)
  a <- gsub("yes", "1", a)
  a <- gsub("M","0", a)
  a <- gsub("F", "1", a)
  a <- gsub("Commercial","0", a)
  a <- gsub("Private", "1", a)
  a <- gsub("Rural", "0", a)
  a <- gsub("Urban","1", a)
  a <- as.numeric(a)
  return(a)
}

#use this when a "YES" answer is a bad thing, like prior accidents or license revoked
sToBLN_Reverse <- function(a){
  a <- gsub("No","1", a)
  a <- gsub("Yes", "0", a)
  a <- gsub("no","1", a)
  a <- gsub("yes", "0", a)
  a <- as.numeric(a)
  return(a)
}



#MAKE A COPY OF THE IMPORTED TRAINING DATA FRAME
insc <- ins 

#clean up unfriendly values
insc$MSTATUS    <- str_replace(insc$MSTATUS, pattern = "z_No", replacement = "No")
insc$SEX        <- str_replace(insc$SEX, pattern = "z_F", replacement = "F")
insc$EDUCATION  <- str_replace(insc$EDUCATION, pattern = "z_High School", replacement = "Secondary")
insc$EDUCATION  <- str_replace(insc$EDUCATION, pattern = "<High School", replacement = "Primary")
insc$JOB        <- str_replace(insc$JOB, pattern = "z_Blue Collar", replacement = "Tradesperson")
insc$CAR_TYPE   <- str_replace(insc$CAR_TYPE, pattern = "z_SUV", replacement = "SUV")
insc$URBANICITY <- str_replace(insc$URBANICITY, pattern = "Highly Urban/ Urban", replacement = "Urban")
insc$URBANICITY <- str_replace(insc$URBANICITY, pattern = "z_Highly Rural/ Rural", replacement = "Rural")

insc$CAR_AGE[insc$CAR_AGE < 0 ] <- 0  #If car age is below zero, set to zero
insc$TARGET_AMT <- round(insc$TARGET_AMT,digits = 0) #Round Target Claim to nearest dollar

#Convert data that is errantly classed as character to numeric: INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM
insc$TARGET_FLAG <- sToNum(insc$TARGET_FLAG)
insc$INCOME      <- sToNum(insc$INCOME)
insc$HOME_VAL    <- sToNum(insc$HOME_VAL)
insc$BLUEBOOK    <- sToNum(insc$BLUEBOOK)
insc$OLDCLAIM    <- sToNum(insc$OLDCLAIM)

#create numeric representations of the "binary choice" variables so they can be support multiple imputation (filling in NA values)
insc$blnPARENT1     <- sToBLN(insc$PARENT1)
insc$blnMSTATUS     <- sToBLN(insc$MSTATUS)
insc$blnSEX         <- sToBLN(insc$SEX)
insc$blnCAR_USE     <- sToBLN(insc$CAR_USE)
insc$blnNOT_RED_CAR <- sToBLN_Reverse(insc$RED_CAR) #Since having a red car increases likelihood of accident, use reverse boolean
insc$blnNOT_REVOKED <- sToBLN_Reverse(insc$REVOKED)
insc$blnURBANICITY  <- sToBLN(insc$URBANICITY)

#CREATE numeric representations of the categorical variables (to support multiple imputation) 
#Education: 1-Primary, 2-Secondary, 3-Bachelors, 4-Masters, 5-PhD
educ <- data.frame(EDUCATION = c("Primary","Secondary","Bachelors","Masters","PhD"), intEDUCATION = c(1,2,3,4,5))
insc$intEDUCATION <- educ$intEDUCATION[match(insc$EDUCATION,educ$EDUCATION)]
#Job - 1 is lowest income, 8 is highest:
job <- data.frame(JOB = c("Student","Home Maker","Clerical","Tradesperson","Professional","Manager","Lawyer","Doctor"), intJOB = c(1,2,3,4,5,6,7,8))
insc$intJOB <- job$intJOB[match(insc$JOB, job$JOB)]
#Vehicle Type  by avg blue book: 1-Sports Car has lowest blue book, Panel Truck has highest
typ <- data.frame(CAR_TYPE = c("Sports Car","SUV","Pickup","Minivan","Van","Panel Truck"), intCAR_TYPE = c(1,2,3,4,5,6)) 
insc$intCAR_TYPE <- typ$intCAR_TYPE[match(insc$CAR_TYPE, typ$CAR_TYPE)]

summary(insc) #Show the new attributes and corrections

#mice PACKAGE
#uses Predictive Mean Matching. 
mice.i <- mice(insc, m = 3, print=F)
mice.ic <- complete(mice.i,1)

insi <- subset(mice.ic,select = c("INDEX", "TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

head(insi)
summary(insi)  #  <---- USE insi DATA FRAME for modeling with imputed data  ***
#conclusion: the imputed values are above zero.  wish I could see methods, R-Squares 

#now we must make a dataframe with numeric-only fields
insi <- subset(mice.ic,select = c("INDEX", "TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

head(insi)


```




Below is a summary of each predictor variable's basic statistics, followed by boxplots which illustrate the spread and outliers for each variable.    


```{r,echo=FALSE,warning=FALSE}
# First, remove the INDEX variable column since it's not used
insi <- insi[2:length(insi)]
# Next, get a general look at the data
# Let's start by exploring the type of each variable
types <- sapply(1:length(insi),function(x) typeof(insi[,x]))
types.df <- data.frame(VAR=names(insi),TYPE=types)
kable(types.df)

# Now generate some summary statistics
kable(summary(insi[1:6]))
kable(summary(insi[7:12]))
kable(summary(insi[13:18]))
kable(summary(insi[19:length(insi)]))

# Look over the variables checking for outliers/influential points, correlation between variables, etc. using box plots.
#
# set the plot-page configuration

```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$KIDSDRIV, main="KIDSDRIV")
boxplot(insi$AGE, main="AGE")
boxplot(insi$HOMEKIDS, main="HOMEKIDS")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$YOJ, main="YOJ")
boxplot(insi$HOME_VAL, main="HOME_VAL")
boxplot(insi$intEDUCATION, main="EDUCATION")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$intJOB, main="JOB")
boxplot(insi$INCOME, main="INCOME")
boxplot(insi$TRAVTIME, main="TRAVTIME")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$TIF, main="TIF")
boxplot(insi$intCAR_TYPE, main="CAR_TYPE")
boxplot(insi$BLUEBOOK, main="BLUEBOOK")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$OLDCLAIM, main="OLDCLAIM")
boxplot(insi$CLM_FREQ, main="CLM_FREQ")
boxplot(insi$MVR_PTS, main="MVR_PTS")
```

```{r,echo=FALSE,warning=FALSE,fig.height = 3}
#par(mfcol=c(1,3))
boxplot(insi$CAR_AGE, main="CAR_AGE")
```
   
```{r,echo=FALSE,warning=FALSE}
# Lastly, build a correlation table to identify any relationships between the predictor variables
# get a table of only predictor variables
insi.pred <- subset(insi,select=-c(TARGET_FLAG,TARGET_AMT))

cor.table <- cor(insi.pred) # build a table of inter-variable correlation values
kable(cor.table[,1:8])
kable(cor.table[,9:16])
```
  
Here are the results from an analysis of the predictor variable correlations:  
  
The are no strong correlations (>70%) between predictor variables, not enough to allow consideration of removing a variable from the model based on a high correlation with another variable. There is some moderate correlation (30-50%) between some variable highlighting obvious relationships such as HOMEKIDS-KIDSDRIV, HOME_VAL-INCOME, EDUCATION-INCOME, JOB-INCOME, CAR_TYPE-BLUEBOOK, CLM_FREQ-OLDCLAIM, and MVR_PTS-CLM_FREQ.  
  
Based on an analysis of the box plots, the following variables have some outliers that may, or may not, exert influence on the regression results: 
    - KIDSDRIV, HOME_VAL, TRAVTIME, MVR_PTS, AGE, INCOME, BLUEBOOK, OLDCLAIM  
  
We'll next look at these variables more closely, starting with their histograms and frequency counts to better understand the nature of their distribution.  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# KIDSDRIV
m <- mean(insi$KIDSDRIV)
s <- sd(insi$KIDSDRIV)
par(mfcol=c(1,3))
hist(insi$KIDSDRIV,prob=TRUE,xlab="KIDSDRIV",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# zn is so skewed, let's look at a frequency count
plot(table(insi$KIDSDRIV))
# let's look at a plot of the values
plot(insi$KIDSDRIV)

```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# HOME_VAL
m <- mean(insi$HOME_VAL)
s <- sd(insi$HOME_VAL)
par(mfcol=c(1,3))
hist(insi$HOME_VAL,prob=TRUE,xlab="HOME_VAL",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
#let's look at a frequency count
plot(table(insi$HOME_VAL))
# let's look at a plot of the values
plot(insi$HOME_VAL)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# MVR_PTS
m <- mean(insi$MVR_PTS)
s <- sd(insi$MVR_PTS)
par(mfcol=c(1,3))
hist(insi$MVR_PTS,prob=TRUE,xlab="MVR_PTS",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
#let's look at a frequency count
plot(table(insi$MVR_PTS))
# let's look at a plot of the values
plot(insi$MVR_PTS)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# TRAVTIME
m <- mean(insi$TRAVTIME)
s <- sd(insi$TRAVTIME)
par(mfcol=c(1,3))
hist(insi$TRAVTIME,prob=TRUE,xlab="TRAVTIME",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# TRAVTIME is so skewed, let's look at a frequency count
plot(table(insi$TRAVTIME))
# let's look at a plot of the values
plot(insi$TRAVTIME)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# AGE
m <- mean(insi$AGE)
s <- sd(insi$AGE)
par(mfcol=c(1,3))
hist(insi$AGE,prob=TRUE,xlab="AGE",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# AGE is so skewed, let's look at a frequency count
plot(table(insi$AGE))
# let's look at a plot of the values
plot(insi$AGE)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# INCOME
m <- mean(insi$INCOME)
s <- sd(insi$INCOME)
par(mfcol=c(1,3))
hist(insi$INCOME,prob=TRUE,xlab="INCOME",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# INCOME is so skewed, let's look at a frequency count
plot(table(insi$INCOME))
# let's look at a plot of the values
plot(insi$INCOME)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# BLUEBOOK
m <- mean(insi$BLUEBOOK)
s <- sd(insi$BLUEBOOK)
par(mfcol=c(1,3))
hist(insi$BLUEBOOK,prob=TRUE,xlab="BLUEBOOK",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# BLUEBOOK is so skewed, let's look at a frequency count
plot(table(insi$BLUEBOOK))
# let's look at a plot of the values
plot(insi$BLUEBOOK)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# OLDCLAIM
m <- mean(insi$OLDCLAIM)
s <- sd(insi$OLDCLAIM)
par(mfcol=c(1,3))
hist(insi$OLDCLAIM,prob=TRUE,xlab="OLDCLAIM",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# OLDCLAIM is so skewed, let's look at a frequency count
plot(table(insi$OLDCLAIM))
# let's look at a plot of the values
plot(insi$OLDCLAIM)
```  
  
  
The analysis of the distributions for these variables show varying degrees of skewness, except for the AGE variable, which shows a fairly normal distribution.  
  
For the logistic regression analysis, we would like to remove as much of the skewness as possible from the candidate predictor variables. A transformation analysis was performed and a log transformation of most of the skewed variables result in a near-normal distribution consequently, for the logistic regression model, we will use the log value of the following variables for the modeling: OLDCLAIM, BLUEBOOK, TRAVTIME, HOME_VAL.  
  
According to the description, the variables *zn*, *indus*, and *age* are area, or land, proportions. According to the statistical summary, the values for these variables are all within the range [1,100] that we would expect.  
  
Based on our detailed review of the variables that contained outliers, the following variables could be problematic:  
  
The predictor variable *zn* is highly right skewed, we can confirm this by comparing the median and mean where the median is 0.0, but the mean is 11.58. The frequency count plot shows how poor the distribution is due to clustering of the data at one extreme.  
  
The predictor variable *black* is highly left skewed. We can confirm this by comparing the median and mean where the median is 391.34 and the mean is 357.12. The frequency count plot shows how poor the distribution is due to clustering of the data at one extreme.  
  
The predictor variable *dis* is slightly right skewed. We can confirm this by comparing the median and mean where the median is 3.191 and the mean is 3.796.  
  
Fortunately, no missing data, or NAs, were found.  
  
The following data corrections were identified in this section:  
  
(1) The predictor variable *chas* and the response variable *target* are categorical (binary), so we need to convert them to factors.  
  
(2) Need to determine if there are other variables highly coorelated with the *zn* or *black* variables that do not have the severe skew and outliers. This could allow us to remove the *zn* or *black* variables from the model.  


\begin{center}
{\huge Data Preparation}
\end{center}

--------

##Data Preparation


The variable changes we identified so far include converting the predictor variable *chas* and the response variable *target* to factors.  Next we will look at how each variable correlates to all the others:  

  
```{r,echo=FALSE,warning=FALSE}
# perform the log transformation for the selected predictor variables and add to the data frame
# no variables used in the log transform can have a value of 0

OLDCLAIM.median <- mean(insi$OLDCLAIM)
HOME_VAL.median <- median(insi$HOME_VAL)
BLUEBOOK.median <- median(insi$BLUEBOOK)
TRAVTIME.median <- median(insi$TRAVTIME)

insi$OLDCLAIM[insi$OLDCLAIM<=0] <- OLDCLAIM.median
insi$HOME_VAL[insi$HOME_VAL<=0] <- HOME_VAL.median

insi$logOLDCLAIM <- log(insi$OLDCLAIM)
insi$logBLUEBOOK <- log(insi$BLUEBOOK)
insi$logTRAVTIME <- log(insi$TRAVTIME)
insi$logHOME_VAL <- log(insi$HOME_VAL)

```
  

\begin{center}
{\huge Build Models}
\end{center}

--------

##Build Models

One method of developing multiple regression models is to take a stepwise approach.  To accomplish this, we combine our knowledge from the data exploration above with logistic regression. Univariate Logistic Regression is a useful method to understand how each predictor variable interacts individually with the target (response) variable. Looking at various statistics, we determine which variable may impact our target the most.
  
###Logistic Regression Models  
  
```{r,echo=FALSE,warning=FALSE}
# build the data frame to be used for logistic regression modeling
insi.log <- insi[,!(names(insi) %in% c("OLDCLAIM","TRAVTIME","BLUEBOOK","HOME_VAL"))]

## 80% of the sample size
set.seed(121)
smp_size <- floor(0.80 * nrow(insi.log))

## set the seed to make your partition reproductible
train_ind <- sample(seq_len(nrow(insi.log)), size = smp_size)

train <- insi.log[train_ind, ]
test <- insi.log[-train_ind, ]

# quick look at model with all variables

KIDSDRIV_model<- glm(TARGET_FLAG ~ KIDSDRIV,family=binomial(link='logit'),data=train)           
HOMEKIDS_model<- glm(TARGET_FLAG ~ HOMEKIDS,family=binomial(link='logit'),data=train)       
AGE_model<- glm(TARGET_FLAG ~ AGE,family=binomial(link='logit'),data=train)        
blnPARENT1_model<- glm(TARGET_FLAG ~ blnPARENT1,family=binomial(link='logit'),data=train)        
logHOME_VAL_model<- glm(TARGET_FLAG ~ logHOME_VAL,family=binomial(link='logit'),data=train)          
logBLUEBOOK_model<- glm(TARGET_FLAG ~ logBLUEBOOK,family=binomial(link='logit'),data=train)          
INCOME_model<- glm(TARGET_FLAG ~ INCOME,family=binomial(link='logit'),data=train)            
CAR_AGE_model<- glm(TARGET_FLAG ~ CAR_AGE,family=binomial(link='logit'),data=train)           
intEDUCATION_model<- glm(TARGET_FLAG ~ intEDUCATION,family=binomial(link='logit'),data=train)            
intJOB_model<- glm(TARGET_FLAG ~ intJOB,family=binomial(link='logit'),data=train)      
blnMSTATUS_model<- glm(TARGET_FLAG ~ blnMSTATUS,family=binomial(link='logit'),data=train)       
intCAR_TYPE_model<- glm(TARGET_FLAG ~ intCAR_TYPE,family=binomial(link='logit'),data=train)       
logOLDCLAIM_model<- glm(TARGET_FLAG ~ logOLDCLAIM,family=binomial(link='logit'),data=train)       
CLM_FREQ_model<- glm(TARGET_FLAG ~ CLM_FREQ,family=binomial(link='logit'),data=train)       
TIF_model<- glm(TARGET_FLAG ~ TIF,family=binomial(link='logit'),data=train)       
MVR_PTS_model<- glm(TARGET_FLAG ~ MVR_PTS,family=binomial(link='logit'),data=train)
blnSEX_model<- glm(TARGET_FLAG ~ blnSEX,family=binomial(link='logit'),data=train)
blnCAR_USE_model<- glm(TARGET_FLAG ~ blnCAR_USE,family=binomial(link='logit'),data=train)
blnNOT_RED_CAR_model<- glm(TARGET_FLAG ~ blnNOT_RED_CAR,family=binomial(link='logit'),data=train)
blnNOT_REVOKED_model<- glm(TARGET_FLAG ~ blnNOT_REVOKED,family=binomial(link='logit'),data=train)
blnURBANICITY_model<- glm(TARGET_FLAG ~ blnURBANICITY,family=binomial(link='logit'),data=train)
logTRAVTIME_model<- glm(TARGET_FLAG ~ logTRAVTIME,family=binomial(link='logit'),data=train)

models <- list(KIDSDRIV_model,HOMEKIDS_model,AGE_model,blnPARENT1_model,logBLUEBOOK_model,INCOME_model,
               CAR_AGE_model,intEDUCATION_model,intJOB_model,blnMSTATUS_model,intCAR_TYPE_model,
               CLM_FREQ_model,TIF_model,MVR_PTS_model,blnSEX_model,blnCAR_USE_model,blnNOT_RED_CAR_model,
               blnNOT_REVOKED_model,blnURBANICITY_model,logTRAVTIME_model,logOLDCLAIM_model,
               logHOME_VAL_model)
```
  
```{r,echo=FALSE,warning=FALSE}
#Creation of lists 
var <- c()
p_val <- c()
aic <- c()
auc <- c()

cols <- ncol(insi.log)
#For loop to run variables over univarate glms
for(i in models){
  var <- c(var,variable.names(i)[2])
  aic <- c(aic,i$aic)
  p_val <- c(p_val,summary(i)$coef[2, "Pr(>|z|)"])
  p <- predict(i, newdata=subset(test,select=c(3:cols)), type="response")
  pr <- prediction(p, test$TARGET_FLAG)
  prf <- performance(pr, measure = "tpr", x.measure = "fpr")
  auc_perf<- performance(pr, measure = "auc")
  auc_val <- auc_perf@y.values[[1]]
  auc <- c(auc,auc_val)
}

kable(data.frame(var,p_val,aic,auc))
```

Here we see the selected output criteria for the linear models run with only a single predictor variable. We examine the p-value (significance), the AIC statistic (goodness-of-fit) and the AUC (Area Under Curve) to measure the potential predictive value of each variable, so we can decide whether or not to include it in our multiple regression model.  We are looking for p-values below .05, AIC values as low as possible, and AUC values as high as possible.

From the above table, we can see that *chas* is the least likely to produce any meaningful inference because its p-value is well above .05 (not significant), it has the highest AIC (518, where 100 is considered excellent), and the lowest AUC (.54, where random chance would yield .50).  Therefore, *chas* is the most likely candidate to be removed from our model.  



### Model 1 

As a baseline, we start with a multiple logistic regression model that includes every predictor variable:

In this model-and in all the models- we set aside 20% of the training data and use 80% to train the model we then use the model to predict the outcome of the remaining 20% of the data.  The model yields an Area Under Curve of .95, meaning it chose correctly 95% of the time.


### Model 2
  
In this scenario we attempt to create the simplest model possible by using only one variable - the one that provides the highest overall AUC (performance) by itself.  We calculate AUC for each variable separately and then select the highest result. 


By combining three variables - *nox*, *rad* and *zn* - that is, the concentration of nitrogen oxides, access to radial highways and the proportion of land zoned for large lots, we can predict with 94% accuracy whether the crime rate at this property is above or below average.  Since this is very close to the performance of the model using all variables (95%), we can be confident in using these three variables for our decision support process, and disregarding the others.  
  


#### MODEL 3 - Backward regression starting with all variables
```{r,echo=FALSE,warning=FALSE}
# Simple backward regression
model_3 <- glm(TARGET_FLAG ~ .-TARGET_AMT,family=binomial(link='logit'),data=train)
backward <- step(model_3)
summary(backward)

p <- predict(backward, newdata=subset(test,select=c(3:cols)), type="response")
pr <- prediction(p, test$TARGET_FLAG)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
abline(a = 0, b = 1)


#
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc


m3_prediction <- ifelse(p > 0.5, 1, 0)
confusion_m3 <- confusionMatrix(data = m3_prediction, reference = test[,1])
confusion_m3$table
```

The model reduces to nine variables and yields a nice low residual deviance of 133.9, compared to a null deviance of 515.3.  This roughly means that the model eliminates about 80% of the error compared to choosing at random.  The AUC is .947 which is roughly the same as the full model using all variables.  



Below is table illustrating the various fitness parameters that describe the effectiveness of the models.  All the models are good - from a practical perspective, there is no difference between them.  

```{r,echo=FALSE,warning=FALSE}
# function to calculate the F1 score
f1 <- function(s,p) {
  (2*p*s)/(p+s)
}

Parameters <- c("Accuracy", "Classification Error Rate", "Precision", "Sensitivity", "Specificity", "F1 Score")

Model3 <- c(confusion_m3$overall["Accuracy"], 1 - confusion_m3$overall["Accuracy"], confusion_m3$byClass["Pos Pred Value"], confusion_m3$byClass["Sensitivity"], confusion_m3$byClass["Specificity"], f1(confusion_m3$byClass["Sensitivity"],confusion_m3$byClass["Pos Pred Value"]))

model_summary <- data.frame(Parameters, Model3)

kable(model_summary)

```



\begin{center}
{\huge Choose Model}
\end{center}


--------

##Choose Model


We like *Model 3 With Nox* the best because it eliminates some of the questionable variables - the ones with high skew and many outliers, also it eliminates the chas variable, which was shown earlier as being insignificant.  Ridding the model of these variables helps provide insurance against poor decisions that could arise, *even if they do not show up in the model.* 

#### MODEL 3 WITH NOX VARIABLE USING FULL DATASETS
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3,eval=FALSE}
# recreate the model using the same variables, but using the full dataset
model_3 <- glm(target ~ zn+nox+age+dis+rad+tax+ptratio+black+medv,family=binomial(link='logit'),data=crime)
summary(model_3)
p <- predict(model_3, newdata=subset(crime_eval,select=c(1,4,6,7,8,9,10,11,13)), type="response")

#predict.prob <- 1/exp(p)
#predict.prob <- 1/(1+exp(-p))
predict.prob <- p
predict.result <- ifelse(predict.prob>=.50, 1, 0)

crime_eval <- cbind(crime_eval,predict.prob,predict.result)
kable(crime_eval[,-c(2,3,5,12)])

```


The Smooth Operators of R Fusion Have Struck Again.