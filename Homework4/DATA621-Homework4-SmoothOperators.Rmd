---
title: "DATA621-Homework4-SmoothOperators"
author: "Rob Hodde, Matt Farris, Jeffrey Burmood, Bin Lin"
date: "4/17/2017"
output:
  pdf_document: default
  html_document: default
---  
  
###Problem Description

The objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. 
  
Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.  

Using the training data set, evaluate the multiple linear regression model based on (a) mean squared error, (b) R2, (c) F-statistic, and (d) residual plots. For the binary logistic regression model, will use a metric such as log likelihood, AIC, or ROC curve.  Using the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.  
  
Approach Steps:
  
1) Build a logistic regression model based on the TARGET_FLAG response variable.  
  
2) Generate TARGET_FLAG predictions using the logistic regression model.  
  
3) Build a linear regression model based on the non-zero values of the TARGET_AMT response variable.  
  
4) Generate TARGET_AMT predictions using the linear regression model based on the non-zero values of the predicted TARGET_FLAG variable.  
  
  
\begin{center}
{\huge Data Exploration}
\end{center}

---

##Data Exploration

  
```{r,echo=FALSE,include=FALSE}
# Load required libraries
suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(ROCR)))
suppressWarnings(suppressMessages(library(RCurl)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(Hmisc)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(stringr)))
suppressWarnings(suppressMessages(library(mice)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(reshape2)))

#
# Read in the dataset from github
ins <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework4/data/insurance_training_data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)
ins_eval <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework4/data/insurance-evaluation-data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)
```
  

The first steps in our process was to explore our data. During this exploration, we immediately noticed the presence of inconsistent data, which is why we employed the use of the MICE package to provide weighted mean data to any missing values. 


Our next step was to change the categorical data into numeric values, which was accomplished using the following legend:
\begin{center}
![](https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework4/New_Variables.png)
\end{center}

After that, we repaired the offending values, standardized the data types, and add new variables to quantitatively represent the binary and categorical choices available in the data:

  
```{r,echo=FALSE,warning=FALSE}

#DATA CLEANSING

#FUNCTIONS---------------
#convert character values to numeric 
sToNum <- function(a){
  a <- gsub("[$]","", a)
  a <- gsub("[,]","", a)
  a <- as.numeric(a)
  return(a)
}
#convert binary choices to numeric 1,0.   1 being the less likely to be in accident
sToBLN <- function(a){
  a <- gsub("No","0", a)
  a <- gsub("Yes", "1", a)
  a <- gsub("no","0", a)
  a <- gsub("yes", "1", a)
  a <- gsub("M","0", a)
  a <- gsub("F", "1", a)
  a <- gsub("Commercial","0", a)
  a <- gsub("Private", "1", a)
  a <- gsub("Rural", "0", a)
  a <- gsub("Urban","1", a)
  a <- as.numeric(a)
  return(a)
}

#use this when a "YES" answer is a bad thing, like prior accidents or license revoked
sToBLN_Reverse <- function(a){
  a <- gsub("No","1", a)
  a <- gsub("Yes", "0", a)
  a <- gsub("no","1", a)
  a <- gsub("yes", "0", a)
  a <- as.numeric(a)
  return(a)
}

#MAKE A COPY OF THE IMPORTED TRAINING DATA FRAME
insc <- ins 

#clean up unfriendly values
insc$MSTATUS    <- str_replace(insc$MSTATUS, pattern = "z_No", replacement = "No")
insc$SEX        <- str_replace(insc$SEX, pattern = "z_F", replacement = "F")
insc$EDUCATION  <- str_replace(insc$EDUCATION, pattern = "z_High School", replacement = "Secondary")
insc$EDUCATION  <- str_replace(insc$EDUCATION, pattern = "<High School", replacement = "Primary")
insc$JOB        <- str_replace(insc$JOB, pattern = "z_Blue Collar", replacement = "Tradesperson")
insc$CAR_TYPE   <- str_replace(insc$CAR_TYPE, pattern = "z_SUV", replacement = "SUV")
insc$URBANICITY <- str_replace(insc$URBANICITY, pattern = "Highly Urban/ Urban", replacement = "Urban")
insc$URBANICITY <- str_replace(insc$URBANICITY, pattern = "z_Highly Rural/ Rural", replacement = "Rural")

insc$CAR_AGE[insc$CAR_AGE < 0 ] <- 0  #If car age is below zero, set to zero
insc$TARGET_AMT <- round(insc$TARGET_AMT,digits = 0) #Round Target Claim to nearest dollar

#Convert data that is errantly classed as character to numeric: INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM
insc$TARGET_FLAG <- sToNum(insc$TARGET_FLAG)
insc$INCOME      <- sToNum(insc$INCOME)
insc$HOME_VAL    <- sToNum(insc$HOME_VAL)
insc$BLUEBOOK    <- sToNum(insc$BLUEBOOK)
insc$OLDCLAIM    <- sToNum(insc$OLDCLAIM)

#create numeric representations of the "binary choice" variables so they can be support multiple imputation (filling in NA values)
insc$blnPARENT1     <- sToBLN(insc$PARENT1)
insc$blnMSTATUS     <- sToBLN(insc$MSTATUS)
insc$blnSEX         <- sToBLN(insc$SEX)
insc$blnCAR_USE     <- sToBLN(insc$CAR_USE)
insc$blnNOT_RED_CAR <- sToBLN_Reverse(insc$RED_CAR) #Since having a red car increases likelihood of accident, use reverse boolean
insc$blnNOT_REVOKED <- sToBLN_Reverse(insc$REVOKED)
insc$blnURBANICITY  <- sToBLN(insc$URBANICITY)

#CREATE numeric representations of the categorical variables (to support multiple imputation) 
#Education: 1-Primary, 2-Secondary, 3-Bachelors, 4-Masters, 5-PhD
educ <- data.frame(EDUCATION = c("Primary","Secondary","Bachelors","Masters","PhD"), intEDUCATION = c(1,2,3,4,5))
insc$intEDUCATION <- educ$intEDUCATION[match(insc$EDUCATION,educ$EDUCATION)]
#Job - 1 is lowest income, 8 is highest:
job <- data.frame(JOB = c("Student","Home Maker","Clerical","Tradesperson","Professional","Manager","Lawyer","Doctor"), intJOB = c(1,2,3,4,5,6,7,8))
insc$intJOB <- job$intJOB[match(insc$JOB, job$JOB)]
#Vehicle Type  by avg blue book: 1-Sports Car has lowest blue book, Panel Truck has highest
typ <- data.frame(CAR_TYPE = c("Sports Car","SUV","Pickup","Minivan","Van","Panel Truck"), intCAR_TYPE = c(1,2,3,4,5,6)) 
insc$intCAR_TYPE <- typ$intCAR_TYPE[match(insc$CAR_TYPE, typ$CAR_TYPE)]

#summary(insc) #Show the new attributes and corrections

#mice PACKAGE
#uses Predictive Mean Matching. 
mice.i <- mice(insc, m = 3, print=F)
mice.ic <- complete(mice.i,1)

insi <- subset(mice.ic,select = c("INDEX", "TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

#head(insi)
#summary(insi)  #  <---- USE insi DATA FRAME for modeling with imputed data  ***
#conclusion: the imputed values are above zero.  

#now we must make a dataframe with numeric-only fields
insi <- subset(mice.ic,select = c("INDEX", "TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

#head(insi)
#insi2 <- insi
```

  
Below is a summary of each predictor variable's basic statistics, followed by boxplots which illustrate the spread and outliers for each variable.    


```{r,echo=FALSE,warning=FALSE}
# First, remove the INDEX variable column since it's not used
insi <- insi[2:length(insi)]
# Next, get a general look at the data
# Let's start by exploring the type of each variable
types <- sapply(1:length(insi),function(x) typeof(insi[,x]))
types.df <- data.frame(VAR=names(insi),TYPE=types)
kable(types.df)

# Now generate some summary statistics
kable(summary(insi[1:6]))
kable(summary(insi[7:12]))
kable(summary(insi[13:18]))
kable(summary(insi[19:21]))
kable(summary(insi[22:length(insi)]))

# Look over the variables checking for outliers/influential points, correlation between variables, etc. using box plots.
#
# set the plot-page configuration

```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$KIDSDRIV, main="KIDSDRIV")
boxplot(insi$AGE, main="AGE")
boxplot(insi$HOMEKIDS, main="HOMEKIDS")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$YOJ, main="YOJ")
boxplot(insi$HOME_VAL, main="HOME_VAL")
boxplot(insi$intEDUCATION, main="EDUCATION")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$intJOB, main="JOB")
boxplot(insi$INCOME, main="INCOME")
boxplot(insi$TRAVTIME, main="TRAVTIME")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$TIF, main="TIF")
boxplot(insi$intCAR_TYPE, main="CAR_TYPE")
boxplot(insi$BLUEBOOK, main="BLUEBOOK")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(insi$OLDCLAIM, main="OLDCLAIM")
boxplot(insi$CLM_FREQ, main="CLM_FREQ")
boxplot(insi$MVR_PTS, main="MVR_PTS")
```

```{r,echo=FALSE,warning=FALSE,fig.height = 3}
par(mfcol=c(1,3))

boxplot(insi$CAR_AGE, main="CAR_AGE")
```
   
```{r,echo=FALSE,warning=FALSE}
# Lastly, build a correlation table to identify any relationships between the predictor variables
# get a table of only predictor variables
insi.pred <- subset(insi,select=-c(TARGET_FLAG,TARGET_AMT))
```

Below is a correlation table illustrating the collinearity of each variable to the others.  

   
```{r,echo=FALSE,warning=FALSE}
# Lastly, build a correlation table to identify any relationships between the predictor variables
# get a table of only predictor variables
insi.pred <- subset(insi,select=-c(TARGET_FLAG,TARGET_AMT))

cor.table <- cor(insi.pred) # build a table of inter-variable correlation values
kable(cor.table[,1:6])
kable(cor.table[,7:12])
kable(cor.table[,13:18])

```
  
Here are the results from an analysis of the predictor variable correlations:  
  
The are no strong correlations (>70%) between predictor variables, not enough to allow consideration of removing a variable from the model based on a high correlation with another variable. There is some moderate correlation (30-50%) between some variable highlighting obvious relationships such as HOMEKIDS-KIDSDRIV, HOME_VAL-INCOME, EDUCATION-INCOME, JOB-INCOME, CAR_TYPE-BLUEBOOK, CLM_FREQ-OLDCLAIM, and MVR_PTS-CLM_FREQ.  

  
Based on an analysis of the box plots, the following variables have some outliers that may, or may not, exert influence on the regression results: 
    - KIDSDRIV, HOME_VAL, TRAVTIME, MVR_PTS, AGE, INCOME, BLUEBOOK, OLDCLAIM  
  
We'll next look at these variables more closely, starting with their histograms and frequency counts to better understand the nature of their distribution.  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# KIDSDRIV
m <- mean(insi$KIDSDRIV)
s <- sd(insi$KIDSDRIV)
par(mfcol=c(1,3))
hist(insi$KIDSDRIV,prob=TRUE,xlab="KIDSDRIV",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# zn is so skewed, let's look at a frequency count
plot(table(insi$KIDSDRIV))
# let's look at a plot of the values
plot(insi$KIDSDRIV)

```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# HOME_VAL
m <- mean(insi$HOME_VAL)
s <- sd(insi$HOME_VAL)
par(mfcol=c(1,3))
hist(insi$HOME_VAL,prob=TRUE,xlab="HOME_VAL",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
#let's look at a frequency count
plot(table(insi$HOME_VAL))
# let's look at a plot of the values
plot(insi$HOME_VAL)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# MVR_PTS
m <- mean(insi$MVR_PTS)
s <- sd(insi$MVR_PTS)
par(mfcol=c(1,3))
hist(insi$MVR_PTS,prob=TRUE,xlab="MVR_PTS",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
#let's look at a frequency count
plot(table(insi$MVR_PTS))
# let's look at a plot of the values
plot(insi$MVR_PTS)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# TRAVTIME
m <- mean(insi$TRAVTIME)
s <- sd(insi$TRAVTIME)
par(mfcol=c(1,3))
hist(insi$TRAVTIME,prob=TRUE,xlab="TRAVTIME",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# TRAVTIME is so skewed, let's look at a frequency count
plot(table(insi$TRAVTIME))
# let's look at a plot of the values
plot(insi$TRAVTIME)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# AGE
m <- mean(insi$AGE)
s <- sd(insi$AGE)
par(mfcol=c(1,3))
hist(insi$AGE,prob=TRUE,xlab="AGE",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# AGE is so skewed, let's look at a frequency count
plot(table(insi$AGE))
# let's look at a plot of the values
plot(insi$AGE)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# INCOME
m <- mean(insi$INCOME)
s <- sd(insi$INCOME)
par(mfcol=c(1,3))
hist(insi$INCOME,prob=TRUE,xlab="INCOME",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# INCOME is so skewed, let's look at a frequency count
plot(table(insi$INCOME))
# let's look at a plot of the values
plot(insi$INCOME)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# BLUEBOOK
m <- mean(insi$BLUEBOOK)
s <- sd(insi$BLUEBOOK)
par(mfcol=c(1,3))
hist(insi$BLUEBOOK,prob=TRUE,xlab="BLUEBOOK",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# BLUEBOOK is so skewed, let's look at a frequency count
plot(table(insi$BLUEBOOK))
# let's look at a plot of the values
plot(insi$BLUEBOOK)
```  
  
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# OLDCLAIM
m <- mean(insi$OLDCLAIM)
s <- sd(insi$OLDCLAIM)
par(mfcol=c(1,3))
hist(insi$OLDCLAIM,prob=TRUE,xlab="OLDCLAIM",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# OLDCLAIM is so skewed, let's look at a frequency count
plot(table(insi$OLDCLAIM))
# let's look at a plot of the values
plot(insi$OLDCLAIM)
```  
  
  
The analysis of the distributions for these variables show varying degrees of skewness, except for the AGE variable, which shows a fairly normal distribution.  
  
For the logistic regression analysis, we would like to remove as much of the skewness as possible from the candidate predictor variables. A transformation analysis was performed and a log transformation of most of the skewed variables result in a near-normal distribution consequently, for the logistic regression model, we will use the log value of the following variables for the modeling: OLDCLAIM, BLUEBOOK, TRAVTIME, HOME_VAL.  


\begin{center}
{\huge Data Preparation}
\end{center}

--------

##Data Preparation

As stated earlier, most of preparation was done prior to a thorough exploration. The reason being, is that the missing value was such a hinderance to proper exploration.

Here we perform the log transformation of the variables identifed earlier with high skewness. One of the variables, OLDCLAIM, has such a distorted distribution that the log transformation will not be sufficient to make the variable a viable candidate for our model. For the OLDCLAIM variable, the difference between the median and the mean is so large we will not attempt to use the OLDCLAIM variable.

  
```{r}
# perform the log transformation for the selected predictor variables and add to the data frame
# no variables used in the log transform can have a value of 0

HOME_VAL.median <- median(insi$HOME_VAL)
BLUEBOOK.median <- median(insi$BLUEBOOK)
TRAVTIME.median <- median(insi$TRAVTIME)

insi$HOME_VAL[insi$HOME_VAL<=0] <- HOME_VAL.median
insi$BLUEBOOK[insi$BLUEBOOK<=0] <- BLUEBOOK.median
insi$TRAVTIME[insi$TRAVTIME<=0] <- TRAVTIME.median

insi$BLUEBOOK <- log(insi$BLUEBOOK)
insi$TRAVTIME <- log(insi$TRAVTIME)
insi$HOME_VAL <- log(insi$HOME_VAL)
```
  

\begin{center}
{\huge Data Preparation}
\end{center}


##Data Preparation

One method of developing multiple regression models is to take a stepwise approach.  To accomplish this, we combine our knowledge from the data exploration above with logistic regression. Univariate Logistic Regression is a useful method to understand how each predictor variable interacts individually with the target (response) variable. Looking at various statistics, we determine which variable may impact our target the most.
  
### Logistic Regression Model 

In this model-and in all the models- we set aside 20% of the training data and use 80% to train the model we then use the model to predict the outcome of the remaining 20% of the data.  
  
  
```{r,echo=FALSE,warning=FALSE}
# build the data frame to be used for logistic regression modeling
#insi.log <- insi[,!(names(insi) %in% c("OLDCLAIM","TRAVTIME","BLUEBOOK","HOME_VAL"))]

## 80% of the sample size
set.seed(121)
smp_size <- floor(0.80 * nrow(insi))

## set the seed to make your partition reproductible
train_ind <- sample(seq_len(nrow(insi)), size = smp_size)

train <- insi[train_ind, ]
test <- insi[-train_ind, ]
```

  
In this scenario we attempt to create the simplest model possible by using only one variable - the one that provides the highest overall AUC (performance) by itself.  We calculate AUC for each variable separately and then select the highest result.  
  
```{r,echo=FALSE,warning=FALSE}
# build single variable models for analysis

KIDSDRIV_model<- glm(TARGET_FLAG ~ KIDSDRIV,family=binomial(link='logit'),data=train)           
HOMEKIDS_model<- glm(TARGET_FLAG ~ HOMEKIDS,family=binomial(link='logit'),data=train)       
AGE_model<- glm(TARGET_FLAG ~ AGE,family=binomial(link='logit'),data=train)        
blnPARENT1_model<- glm(TARGET_FLAG ~ blnPARENT1,family=binomial(link='logit'),data=train)        
HOME_VAL_model<- glm(TARGET_FLAG ~ HOME_VAL,family=binomial(link='logit'),data=train)          
BLUEBOOK_model<- glm(TARGET_FLAG ~ BLUEBOOK,family=binomial(link='logit'),data=train)          
INCOME_model<- glm(TARGET_FLAG ~ INCOME,family=binomial(link='logit'),data=train)            
CAR_AGE_model<- glm(TARGET_FLAG ~ CAR_AGE,family=binomial(link='logit'),data=train)           
intEDUCATION_model<- glm(TARGET_FLAG ~ intEDUCATION,family=binomial(link='logit'),data=train)            
intJOB_model<- glm(TARGET_FLAG ~ intJOB,family=binomial(link='logit'),data=train)      
blnMSTATUS_model<- glm(TARGET_FLAG ~ blnMSTATUS,family=binomial(link='logit'),data=train)       
intCAR_TYPE_model<- glm(TARGET_FLAG ~ intCAR_TYPE,family=binomial(link='logit'),data=train)       
CLM_FREQ_model<- glm(TARGET_FLAG ~ CLM_FREQ,family=binomial(link='logit'),data=train)       
TIF_model<- glm(TARGET_FLAG ~ TIF,family=binomial(link='logit'),data=train)       
MVR_PTS_model<- glm(TARGET_FLAG ~ MVR_PTS,family=binomial(link='logit'),data=train)
blnSEX_model<- glm(TARGET_FLAG ~ blnSEX,family=binomial(link='logit'),data=train)
blnCAR_USE_model<- glm(TARGET_FLAG ~ blnCAR_USE,family=binomial(link='logit'),data=train)
blnNOT_RED_CAR_model<- glm(TARGET_FLAG ~ blnNOT_RED_CAR,family=binomial(link='logit'),data=train)
blnNOT_REVOKED_model<- glm(TARGET_FLAG ~ blnNOT_REVOKED,family=binomial(link='logit'),data=train)
blnURBANICITY_model<- glm(TARGET_FLAG ~ blnURBANICITY,family=binomial(link='logit'),data=train)
TRAVTIME_model<- glm(TARGET_FLAG ~ TRAVTIME,family=binomial(link='logit'),data=train)

models <- list(KIDSDRIV_model,HOMEKIDS_model,AGE_model,blnPARENT1_model,BLUEBOOK_model,INCOME_model,
               CAR_AGE_model,intEDUCATION_model,intJOB_model,blnMSTATUS_model,intCAR_TYPE_model,
               CLM_FREQ_model,TIF_model,MVR_PTS_model,blnSEX_model,blnCAR_USE_model,blnNOT_RED_CAR_model,
               blnNOT_REVOKED_model,blnURBANICITY_model,TRAVTIME_model,
               HOME_VAL_model)

#Creation of lists 
var <- c()
p_val <- c()
aic <- c()
auc <- c()
  
cols <- ncol(insi)
#For loop to run variables over univarate glms
for(i in models){
    var <- c(var,variable.names(i)[2])
    aic <- c(aic,i$aic)
    p_val <- c(p_val,summary(i)$coef[2, "Pr(>|z|)"])
    p <- predict(i, newdata=subset(test,select=c(3:cols)), type="response")
    pr <- prediction(p, test$TARGET_FLAG)
    prf <- performance(pr, measure = "tpr", x.measure = "fpr")
    auc_perf<- performance(pr, measure = "auc")
    auc_val <- auc_perf@y.values[[1]]
    auc <- c(auc,auc_val)
 }
 
# build a table of the model analysis results
kable(data.frame(var,p_val,aic,auc))
```
  
  
The highest AUC value obtained is .63, from the variable CLM_FREQ (Claim Frequency), indicating that clients with higher past claim incidents are more likely to have claims in the future.  However, .63 is not a long ways from .50, so this model is not very strong.    
  
  
Next we will derive a logistic regression model by stepping backward from using all candidate variables and arriving at the variable set that maximizes the AUC value.  
  
#### MODEL 1 - Backward regression starting with all variables
```{r,echo=FALSE,warning=FALSE}
# Simple backward regression
model_1 <- glm(TARGET_FLAG ~ .-TARGET_AMT-OLDCLAIM,family=binomial(link='logit'),data=train)
model_1_backward <- step(model_1)
summary(model_1_backward)

p <- predict(model_1_backward, newdata=subset(test,select=c(3:cols)), type="response")
pr <- prediction(p, test$TARGET_FLAG)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
abline(a = 0, b = 1)

#
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc


m1_prediction <- ifelse(p > 0.5, 1, 0)
confusion_m1 <- confusionMatrix(data = m1_prediction, reference = test[,1])
confusion_m1$table
```
  
Our derived logistic regression model has a maximize AUC value of .79 with great p-values on all of the selected variables.  
  
The table below illustrates the various fitness parameters that describe the effectiveness of the logistic regression model.  

```{r,echo=FALSE,warning=FALSE}
# function to calculate the F1 score
f1 <- function(s,p) {
  (2*p*s)/(p+s)
}

Parameters <- c("Accuracy", "Classification Error Rate", "Precision", "Sensitivity", "Specificity", "F1 Score")

Model1 <- c(confusion_m1$overall["Accuracy"], 1 - confusion_m1$overall["Accuracy"], confusion_m1$byClass["Pos Pred Value"], confusion_m1$byClass["Sensitivity"], confusion_m1$byClass["Specificity"], f1(confusion_m1$byClass["Sensitivity"],confusion_m1$byClass["Pos Pred Value"]))

model_summary <- data.frame(Parameters, Model1)

kable(model_summary)
```

###Linear Regression Models  

For the linear regression model, we went ahead and attempted to produce several models to try and find the cost of repair based on several factors. Unlike in the Logistic Model, where we utilized all the variables and work backwards, for the linear approach we decided to limit to 8 variables, which most "intuitively" made sense to include when trying to predict cost. Certain variables, even when made into numeric values, just would not make any sense to predict cost. For instance, all binary data was eliminated from this model, it just didn't seem reasonable to predict a continous value using these independent variables. So, we started modeling from these Variables:
"TARGET_FLAG","INCOME","TRAVTIME","BLUEBOOK", "YOJ","intEDUCATION","intJOB","CAR_AGE"

Our first attempt at modelling will be to take a generic linear model with our two tranformed variables that we kept:

```{r,echo=FALSE,warning=FALSE}
lin_model <- subset(insi, TARGET_FLAG == 1)
Var_lm <- c("TARGET_AMT","TARGET_FLAG","INCOME","TRAVTIME","BLUEBOOK", "YOJ","intEDUCATION","intJOB","CAR_AGE")
```

```{r,echo=FALSE,warning=FALSE}
model_ols <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                    YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data =lin_model)
summary(model_ols)

plot(fitted(model_ols), residuals(model_ols))
```

From the above, we can see that a straight linear model was not very effective at producing any results.

Aside (Matt's point of view)- At this point, I had tried several different ways to mitigate the problem shown above, and that was that the only variable I saw of any signficance was the BLUEBOOK value. I tried different a differnt transformation type mostly of the log(TARGET_AMT) ~ log(Dependent Variables). I then tried several different weighting types like 1/SD and 1/fitted (shown below), but I still got the same results. Through entirely happenstance, as we were working off the same dataset, one of my compatriates overwrote the variables to transform only two variables with the log function, the TRAVTIME and BLUEBOOK variable. Since I left the code sitting there, lo and behold, the transformations of just those two skewed values changed the entire code. 

```{r,echo=FALSE,warning=FALSE}
wts <- 1/fitted(lm(abs(residuals(model_ols)) ~ fitted(model_ols)))^2

#length(wts)
#length(model_ols$residuals)
model_wls <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                    YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data = lin_model, weights = wts)
summary(model_wls)
plot(fitted(model_wls), residuals(model_wls))
```

Using this weighing value, which weights each variable according to $1/(fitted)^2$. As you can see from the p values and the $R^2$-values our model become much more approproate. Originally we were has a "goodness of fit" measure of 1% barely describing the variance in our model. With the weighting, we now have a much higher $R^2$. 
  

Originally, our model we were going to choose relied on the below, which used log transformations on most of the independent variables AND our independent variable. 
```{r,echo=FALSE,warning=FALSE}
log_var <- c("TARGET_AMT","INCOME")

log_lin_model <- lin_model[Var_lm]
log_lin_model[log_var] <- log(log_lin_model[log_var])
log_lin_model <- do.call(data.frame,lapply(log_lin_model, function(x) replace(x, is.infinite(x),0)))
log_model_ols <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                      YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data =log_lin_model)

summary(log_model_ols)

plot(fitted(log_model_ols), residuals(log_model_ols))

log_wts <- 1/fitted(lm(abs(residuals(log_model_ols)) ~ fitted(log_model_ols)))^2

log_model_wls <- lm(TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK+
                    YOJ + intEDUCATION + intJOB + CAR_AGE, 
                    data = log_lin_model, weights = log_wts)
summary(log_model_wls)

plot(fitted(log_model_wls), rstandard(log_model_wls))

```

As you can see from the statistical output, with a log transformed dependent variable, our model does not hold up. However, or F-statistic is much lower, and the reason for this, is that the F-statistic is not corrected for the weights. 


```{r}
backward <- step(model_wls)
summary(backward)
```
\begin{center}
{\huge Choose Model}
\end{center}

--------

##Choose Model


We chose the Backward logistic regression model to make our predictions for the evaluation dataset (non-zero value). This model has accuracy rate as high as 80%. In the meantime, the precision and sensitivity are at the level of 82% and 92%, which indicate this model is very good at eliminating false negative and false positive situations.  Both AUC and F1 Score are around 80%, which also indicates that it has high accuracy in terms of predicting the final response variables.


```{r, echo=FALSE, warning=FALSE}
#The following is simply the data preparation step. To get evaluation data ready to plug into our model.

eval_copy <- ins_eval
eval_copy$MSTATUS    <- str_replace(eval_copy$MSTATUS, pattern = "z_No", replacement = "No")
eval_copy$SEX        <- str_replace(eval_copy$SEX, pattern = "z_F", replacement = "F")
eval_copy$EDUCATION  <- str_replace(eval_copy$EDUCATION, pattern = "z_High School", replacement = "Secondary")
eval_copy$EDUCATION  <- str_replace(eval_copy$EDUCATION, pattern = "<High School", replacement = "Primary")
eval_copy$JOB        <- str_replace(eval_copy$JOB, pattern = "z_Blue Collar", replacement = "Tradesperson")
eval_copy$CAR_TYPE   <- str_replace(eval_copy$CAR_TYPE, pattern = "z_SUV", replacement = "SUV")
eval_copy$URBANICITY <- str_replace(eval_copy$URBANICITY, pattern = "Highly Urban/ Urban", replacement = "Urban")
eval_copy$URBANICITY <- str_replace(eval_copy$URBANICITY, pattern = "z_Highly Rural/ Rural", replacement = "Rural")

eval_copy$CAR_AGE[eval_copy$CAR_AGE < 0 ] <- 0
eval_copy$TARGET_AMT <- round(eval_copy$TARGET_AMT,digits = 0)

eval_copy$TARGET_FLAG <- sToNum(eval_copy$TARGET_FLAG)
eval_copy$INCOME      <- sToNum(eval_copy$INCOME)
eval_copy$HOME_VAL    <- sToNum(eval_copy$HOME_VAL)
eval_copy$BLUEBOOK    <- sToNum(eval_copy$BLUEBOOK)
eval_copy$OLDCLAIM    <- sToNum(eval_copy$OLDCLAIM)

eval_copy$blnPARENT1     <- sToBLN(eval_copy$PARENT1)
eval_copy$blnMSTATUS     <- sToBLN(eval_copy$MSTATUS)
eval_copy$blnSEX         <- sToBLN(eval_copy$SEX)
eval_copy$blnCAR_USE     <- sToBLN(eval_copy$CAR_USE)
eval_copy$blnNOT_RED_CAR <- sToBLN_Reverse(eval_copy$RED_CAR)
eval_copy$blnNOT_REVOKED <- sToBLN_Reverse(eval_copy$REVOKED)
eval_copy$blnURBANICITY  <- sToBLN(eval_copy$URBANICITY)

educ <- data.frame(EDUCATION = c("Primary","Secondary","Bachelors","Masters","PhD"), intEDUCATION = c(1,2,3,4,5))
eval_copy$intEDUCATION <- educ$intEDUCATION[match(eval_copy$EDUCATION,educ$EDUCATION)]
job <- data.frame(JOB = c("Student","Home Maker", "Clerical", "Tradesperson", "Professional", "Manager", "Lawyer", "Doctor"), intJOB = c(1,2,3,4,5,6,7,8))
eval_copy$intJOB <- job$intJOB[match(eval_copy$JOB, job$JOB)]
typ <- data.frame(CAR_TYPE = c("Sports Car","SUV","Pickup","Minivan","Van","Panel Truck"), intCAR_TYPE = c(1,2,3,4,5,6)) 
eval_copy$intCAR_TYPE <- typ$intCAR_TYPE[match(eval_copy$CAR_TYPE, typ$CAR_TYPE)]

mice.i <- mice(eval_copy, m = 3, print=F)
mice.ic <- complete(mice.i,1)

eval_copy <- subset(mice.ic, select = c("TARGET_FLAG", "TARGET_AMT", "KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "HOME_VAL", "TRAVTIME", "BLUEBOOK", "TIF", "OLDCLAIM", "CLM_FREQ", "MVR_PTS", "CAR_AGE", "blnPARENT1", "blnMSTATUS", "blnSEX", "blnCAR_USE", "blnNOT_RED_CAR", "blnNOT_REVOKED", "blnURBANICITY", "intEDUCATION", "intJOB", "intCAR_TYPE"))

```


```{r, echo=FALSE, warning=FALSE}
# Perform log transformation on the previously identified variables
HOME_VAL.median <- median(eval_copy$HOME_VAL)
BLUEBOOK.median <- median(eval_copy$BLUEBOOK)
TRAVTIME.median <- median(eval_copy$TRAVTIME)

eval_copy$HOME_VAL[eval_copy$HOME_VAL<=0] <- HOME_VAL.median
eval_copy$BLUEBOOK[eval_copy$BLUEBOOK<=0] <- BLUEBOOK.median
eval_copy$TRAVTIME[eval_copy$TRAVTIME<=0] <- TRAVTIME.median

eval_copy$BLUEBOOK <- log(eval_copy$BLUEBOOK)
eval_copy$TRAVTIME <- log(eval_copy$TRAVTIME)
eval_copy$HOME_VAL <- log(eval_copy$HOME_VAL)

# Built the model using the train dataset. Then we save the results under TARGET_FLAG in eval dataset.
# use the backward step model created earlier so comment this code out eve though it's equivalent
#model_3 <- glm(TARGET_FLAG ~ KIDSDRIV + INCOME + TIF + CLM_FREQ + MVR_PTS + blnPARENT1 + blnMSTATUS + blnCAR_USE + blnNOT_REVOKED + blnURBANICITY + intEDUCATION + intJOB + intCAR_TYPE + BLUEBOOK + TRAVTIME + HOME_VAL, family = binomial(link='logit'), data = train)
#summary(model_3)

p <- predict(model_1_backward, newdata=eval_copy, type = "response")

eval_copy$TARGET_FLAG <- ifelse(p >= .50, 1, 0)

```


For linear regression model, we are showing the outputs of each of the 4 tests we had. M1 = straight LM with 2 transformations, m2 = weight lm, m3 = straight lm with log independent, m4 = same as previous but weighted. According to the following summary statistics, the weighted lm out performs all the others. As mentioned previously, the F-statistic is high, which might indicate our model lacks validity. Or it could also indicate the claim amount of motor vehicle accident tends to be unpredictable, and wit
  
#I got a out of bound error for m5, I do not know how to fix it. 

```{r, echo=FALSE, warning=FALSE}
Parameters <- c("p-value", "Mean Squared Error", "R^2", "F-Statistics")

m1 <- c(summary(model_ols)$coef[4, "Pr(>|t|)"], mean(summary(model_ols)$residuals^2), summary(model_ols)$r.squared, summary(model_ols)$fstatistic[1])
m2 <- c(summary(model_wls)$coef[4, "Pr(>|t|)"], mean(summary(model_wls)$residuals^2), summary(model_wls)$r.squared, summary(model_wls)$fstatistic[1])
m3 <- c(summary(log_model_ols)$coef[4, "Pr(>|t|)"], mean(summary(log_model_ols)$residuals^2), summary(log_model_ols)$r.squared, summary(log_model_ols)$fstatistic[1])
m4 <- c(summary(log_model_wls)$coef[4, "Pr(>|t|)"], mean(summary(log_model_wls)$residuals^2), summary(log_model_wls)$r.squared, summary(log_model_wls)$fstatistic[1])


#I got a out of bound error for m5, I do not know how to fix it. 

#m5 <- c(summary(backward)$coef[4, "Pr(>|t|)"], mean(summary(backward)$residuals^2), summary(backward)$r.squared, summary(backward)$fstatistic[1])

model_summary <- data.frame(Parameters, m1, m2, m3, m4)
kable(model_summary)
```


```{r, echo=FALSE, warning=FALSE}
#Build model based on our weighted least square model with wts 
linear_model <- lm(formula = TARGET_AMT ~ INCOME + TRAVTIME + BLUEBOOK + YOJ + 
    intEDUCATION + intJOB + CAR_AGE, data = lin_model, weights = wts)

summary(linear_model)
eval_copy$TARGET_AMT <- predict(linear_model, newdata = eval_copy, type = "response")
```

Below is the evaluation of the first 20 values on the evaluation test dataset. 
```{r, echo=FALSE, warning=FALSE}
kable(head(eval_copy[,c(1:8)], 20))
kable(head(eval_copy[,c(9:16)], 20))
kable(head(eval_copy[,c(17:length(insi))], 20))
```

```{r, echo=FALSE, warning=FALSE}
kable(head(eval_copy[,c(1:10)], 20))
```

The Smooth Operators of R Fusion Have Struck Again.