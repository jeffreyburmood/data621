---
title: "DATA621-Homework3-HoddeFarrisBurmoodLin"
author: "Rob Hodde, Matt Farris, JeffreyBurmood, Bin Lin"
date: "3/28/2017"
output:
  pdf_document: default
  html_document: default
---

\begin{center}
{\huge Problem Description}
\end{center}

Explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Using the data set build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. Provide classifications and probabilities for the evaluation data set using the developed binary logistic regression model.  
  

\begin{center}
{\huge Data Exploration}
\end{center}




```{r,include=FALSE,warning=FALSE}
# Load required libraries
library(ggplot2)
library(ROCR)
library(RCurl)
library(knitr)
library(Hmisc)
#
# Read in the dataset from github
crime <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework3/data/crime-training-data.csv"),header=TRUE,na.strings=c(" "))
crime_eval <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework3/data/crime-evaluation-data.csv"),header=TRUE,na.strings=c(" "))

```



```{r,echo=FALSE,warning=FALSE}
# First, get a general look at the data
#head(crime)
# Let's start by exploring the type of each variable
types <- sapply(1:length(crime),function(x) typeof(crime[,x]))
types.df <- data.frame(VAR=names(crime),TYPE=types)
kable(types.df)
# Now generate some summary statistics
kable(summary(crime[1:6]))
kable(summary(crime[7:12]))
# Visual check for obvious correlations
#pairs(crime,col=crime$target)
#
# no NAs found so no missing values to remove or fix
#
# Look over the variables checking for outliers/influencial points, correlation between variables, etc. using box plots.
#
# set the plot-page configuration

```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(crime$zn, main="zn")
boxplot(crime$indus, main="indus")
boxplot(crime$nox, main="nox")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(crime$rm, main="rm")
boxplot(crime$age, main="age")
boxplot(crime$dis, main="dis")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))

boxplot(crime$rad, main="rad")
boxplot(crime$tax, main="tax")
boxplot(crime$ptratio, main="ptratio")
```

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
par(mfcol=c(1,3))
boxplot(crime$black, main="black")
boxplot(crime$lstat, main="lstat")
boxplot(crime$medv, main="mdev")
```


Based on an analysis of the box plots, the following variables have some outliers that may, or may not, exert influence on the regression results.  
 - zn, rm, dis, black, lstat, medv  
  
We'll next look at these variable mroe closely, starting with there histograms and frequency counts to better understand the nature of their distribution.  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# zn
m <- mean(crime$zn)
s <- sd(crime$zn)
par(mfcol=c(1,3))
hist(crime$zn,prob=TRUE,xlab="ZN",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# zn is so skewed, let's look at a frequency count
plot(table(crime$zn))
# let's look at a plot of the values
plot(crime$zn)

```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# rm
m <- mean(crime$rm)
s <- sd(crime$rm)
par(mfcol=c(1,3))
hist(crime$rm,prob=TRUE,xlab="RM",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# let's look at a plot of the values
plot(crime$rm)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# dis
m <- mean(crime$dis)
s <- sd(crime$dis)
par(mfcol=c(1,3))
hist(crime$dis,prob=TRUE,xlab="DIS",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# let's look at a plot of the values
plot(crime$dis)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# black
m <- mean(crime$black)
s <- sd(crime$black)
par(mfcol=c(1,3))
hist(crime$black,prob=TRUE,xlab="BLACK",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# black is so skewed, let's look at a frequency count
plot(table(crime$black))
# let's look at a plot of the values
plot(crime$black)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# lstat
m <- mean(crime$lstat)
s <- sd(crime$lstat)
par(mfcol=c(1,3))
hist(crime$lstat,prob=TRUE,xlab="LSTAT",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# let's look at a plot of the values
plot(crime$lstat)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# medv
m <- mean(crime$medv)
s <- sd(crime$medv)
par(mfcol=c(1,3))
hist(crime$medv,prob=TRUE,xlab="MEDV",main='')
curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
# let's look at a plot of the values
plot(crime$medv)
```  

```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}


# quick look at model with all variables
crime.model <- glm(target ~ .,family=binomial(link='logit'),data=crime)

```
  
According to the description, the variables zn, indus, and age are area, or land, proportions. According to the statistical summary, the values for these variables are all within the range [1,100] as you would expect.  
  
Based on our detailed review of the variables that contained outliers, the following variables could be problematic:  
  
The predictor variable zn is highly right skewed, we can confirm this by comparing the median and mean where the median is 0.0, but the median is 11.58. The frequency count plot shows how poor the distribution is due to clustering of the data at one extreme.  
  
The predictor variable black is highly left skewed. We can confirm this by comparing the median and mean where the median is 391.34 and the mean is 357.12. The frequency count plot shows how poor the distribution is due to clustering of the data at one extreme.  
  
The predictor variable dis is slightly right skewed. We can confirm this by comparing the median and mean where the median is 3.191 and the mean is 3.796.  
  
Fortunately, no missing data, or NAs, were found.  
  
The following data corrections were identified in this section:  
  
(1) The predictor variable "chas" and the response variable "target" are supposed to be categorical (binary), so we need to convert them to factors.  
  
(2) Need to determine if there are other variables highly coorelated with the zn or black variable that don't have the severe skew and outliers. This would allow us to remove the zn or black variable from the model.  


\begin{center}
{\huge Data Preparation}
\end{center}
The variable changes we identified so far include converting the predictor variable "chas" and the response variable "target" to factors.
  
```{r,echo=FALSE,warning=FALSE}
# Based on the data exploration results, identify any changes, transformations, and new or deleted variables to use for the next step of building the models.
# Need to set variables to a factor as required
crime$target <- as.factor(crime$target)
crime$chas <- as.factor(crime$chas)
crime_eval$chas <- as.factor(crime_eval$chas)

# get a table of non-factor variables
crime.nofactor <- subset(crime,select=-c(chas,target))
# build a correlation table to study the variable relationships
cor.table <- cor(crime.nofactor) # build a table of inter-variable correlation values
kable(cor.table[,1:6])
kable(cor.table[,7:12])
```
  
Based on the correlation table, the variable zn has a moderate correlation with the variable dis. The plot of the dis data shows a much better distribution of values. Consequently, one possibility is to remove the zn variable from the data set for modeling. 


\begin{center}
{\huge Build Models}
\end{center}

One analysis of multiple regression models is to take a stepwise approach, and to begin this step, we first take our knowledge from the data exploration, and combine it with a logistic regression. The Univariate Logistic Regression is a useful tool to understand how each variable plays against our target variable. Looking at various statistics, we can which variable impacts are target the most.

```{r,echo=FALSE,warning=FALSE}
## 80% of the sample size
set.seed(121)
smp_size <- floor(0.80 * nrow(crime))

## set the seed to make your partition reproductible
train_ind <- sample(seq_len(nrow(crime)), size = smp_size)

train <- crime[train_ind, ]
test <- crime[-train_ind, ]

# quick look at model with all variables


zn_uni_model<- glm(target ~ zn,family=binomial(link='logit'),data=train)           
indus_uni_model<- glm(target ~ indus,family=binomial(link='logit'),data=train)       
chas_uni_model<- glm(target ~ chas,family=binomial(link='logit'),data=train)        
nox_uni_model<- glm(target ~ nox,family=binomial(link='logit'),data=train)        
rm_uni_model<- glm(target ~ rm,family=binomial(link='logit'),data=train)          
age_uni_model<- glm(target ~ age,family=binomial(link='logit'),data=train)          
dis_uni_model<- glm(target ~ dis,family=binomial(link='logit'),data=train)            
rad_uni_model<- glm(target ~ rad,family=binomial(link='logit'),data=train)           
tax_uni_model<- glm(target ~ tax,family=binomial(link='logit'),data=train)            
ptratio_uni_model<- glm(target ~ ptratio,family=binomial(link='logit'),data=train)      
black_uni_model<- glm(target ~ black,family=binomial(link='logit'),data=train)       
lstat_uni_model<- glm(target ~ lstat,family=binomial(link='logit'),data=train)       


models <- list(zn_uni_model,indus_uni_model,
            chas_uni_model,nox_uni_model,
            rm_uni_model,age_uni_model,
            dis_uni_model,rad_uni_model,
            tax_uni_model,ptratio_uni_model,
            black_uni_model,lstat_uni_model)

```
  
```{r,echo=FALSE}
#Creation of lists 
var <- c()
p_val <- c()
aic <- c()
auc <- c()

#For loop to run variables over univarate glms
for(i in models){
  var <- c(var,variable.names(i)[2])
  aic <- c(aic,i$aic)
  p_val <- c(p_val,summary(i)$coef[2, "Pr(>|z|)"])
  p <- predict(i, newdata=subset(test,select=c(1,2,3,4,5,6,7,8,9,10,11,12,13)), type="response")
  pr <- prediction(p, test$target)
  prf <- performance(pr, measure = "tpr", x.measure = "fpr")
  auc_perf<- performance(pr, measure = "auc")
  auc_val <- auc_perf@y.values[[1]]
  auc <- c(auc,auc_val)
}

kable(data.frame(var,p_val,aic,auc))
```

We took the p-value, the AIC statistic and then a measure of the Area under the curve to measure the variables potential in a multiple regression model. From the above table, we can see that the chas variable is least likely to be included in our model, as it isn't statistically signficant. From the above table, we can see 1 variable that has no significance and under a univariate regression model, and have high relative AIC, and accuracy that is barely higher than a random variable. The Chas variable is a viable candidate to remove from our modelling.  

# Model 1 

A quick looke at the total model:
```{r,echo=FALSE,warning=FALSE,fig.width = 8, fig.height = 3}
# quick look at model with all variables
## 80% of the sample size
set.seed(121)
smp_size <- floor(0.80 * nrow(crime))
## set the seed to make your partition reproductible
train_ind <- sample(seq_len(nrow(crime)), size = smp_size)

train <- crime[train_ind, ]
test <- crime[-train_ind, ]

crime_model <- glm(target ~ .,family=binomial(link='logit'),data=train)

p <- predict(crime_model, newdata=subset(test,select=c(1:13)), type="response")
pr <- prediction(p, test$target)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
summary(crime_model)
auc  
```

# Model 2
  
We will attempt to create the simplest model possible by using only one variable - the one that provides us the highest overall AUC (performance) all by itself.  We can plug in each variable separately and then select the highest result.  The best variable is nox - the presence of nitrogen oxides (an industrial pollutant) on the property.  
  
```{r,echo=FALSE}
## 75% of the sample size
set.seed(121)
smp_size <- floor(0.80 * nrow(crime))

## set the seed to make your partition reproductible
train_ind <- sample(seq_len(nrow(crime)), size = smp_size)

train <- crime[train_ind, ]
test <- crime[-train_ind, ]

#show the AUC for the most valuable variable - nox (nitrogen oxides concentration - ie: poisoned environment) 
qm <- glm(target ~ nox,family=binomial(link='logit'),data=train)
p <- predict(qm, newdata=subset(test,select=c(4)), type="response")
pr <- prediction(p, test$target)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc  #.934

```
  
By combining nos with all the remaining variables and selecting the highest resulting AUC result, we conclude that nox plus rad (access to radial highways) is the strongest combinaton of two variables.  
  
```{r,echo=FALSE}

#show the AUC for the best combination of two variables - nox and rad
qm <- glm(target ~ nox + rad,family=binomial(link='logit'),data=train)
p <- predict(qm, newdata=subset(test,select=c(4,8)), type="response")
pr <- prediction(p, test$target)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc  #.948

```
  
  
```{r,echo=FALSE}

#show the AUC for the best combination of three variables - nox, rad and zn
qm <- glm(target ~ nox + rad + dis,family=binomial(link='logit'),data=train)
p <- predict(qm, newdata=subset(test,select=c(4,7,8)), type="response")
pr <- prediction(p, test$target)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc  

```
  
By combining three variables - nox, rad and zn - that is, the concentration of nitrogen oxides, access to radial highways and the proportion of land zoned for large lots, we can predict with 95.8% accuracy whether the crime rate at this property is above or below average.  Since this is very close to the performance of the model using all variables (96%), we can be confident in using these three variables for our decision support process, and disregarding the others.  
  

#Model 3

## MODEL 3 WITH NOX VARIABLE
```{r,echo=FALSE,warning=FALSE}
# Simple backward regression
model_3 <- glm(target ~ .,family=binomial(link='logit'),data=train)
backward <- step(model_3)
summary(backward)

p <- predict(backward, newdata=subset(test,select=c(1,2,3,4,5,6,7,8,9,10,11,12,13)), type="response")
pr <- prediction(p, test$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

#
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

```

## MODEL 3 WITHOUT NOX VARIABLE
```{r,echo=FALSE,warning=FALSE}
# Simple backward regression
model_3 <- glm(target ~ .-nox,family=binomial(link='logit'),data=train)
backward <- step(model_3)
summary(backward)

p <- predict(backward, newdata=subset(test,select=c(1,2,3,4,5,6,7,8,9,10,11,12,13)), type="response")
pr <- prediction(p, test$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

#
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

```


All Done!  
  