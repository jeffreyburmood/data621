---
title: "DATA621-HW5-SmoothOperators"
author: "Rob Hodde, Matt Farris, Jeffrey Burmood, Bin Lin"
date: "5/11/2017"
output: pdf_document
---

###Problem Description  
  
Explore, analyze and model a data set containing information on approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine distribution companies after sampling a wine. These cases would be used to provide tasting samples to restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a wine to be sold at a high end restaurant. A large wine manufacturer is studying the data in order to predict the number of wine cases ordered based upon the wine characteristics. If the wine manufacturer can predict the number of cases, then that manufacturer will be able to adjust their wine offering to maximize sales.  
  
The objective is to build a count regression model to predict the number of cases of wine that will be sold given certain properties of the wine. This variable is called TARGET. 
  

\begin{center}
{\huge Data Exploration}
\end{center}
  
##Data Exploration
  
There are numerous NAs in certain variables, and variables with negative values. Variables with negative values have nearly normal distributions so it is possible some previous data adjustments have been made. The variable data with negative values in stable, normal distributions will be used as-is. Below is a summary of variables by type, followed by their basic statistical summaries:  
  
```{r,echo=FALSE}
# Load required libraries

suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(ROCR)))
suppressWarnings(suppressMessages(library(RCurl)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(Hmisc)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(stringr)))
suppressWarnings(suppressMessages(library(mice)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(reshape2)))
suppressWarnings(suppressMessages(library(MASS)))
suppressWarnings(suppressMessages(library(pscl)))
suppressWarnings(suppressMessages(library(broom)))
suppressWarnings(suppressMessages(library(lmtest)))

# Read in the dataset from github
wine.raw <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework5/wine-training-data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)
wine_eval <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Homework5/wine-evaluation-data.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)

# get rid of the INDEX column since it's not used
wine <- wine.raw[2:length(wine.raw)]

# Let's start by exploring the type of each variable
types <- sapply(1:length(wine),function(x) typeof(wine[,x]))
types.df <- data.frame(VAR=names(wine),TYPE=types)
kable(types.df)

# Now generate some summary statistics
kable(summary(wine[1:6]))
kable(summary(wine[7:12]))
kable(summary(wine[13:length(wine)]))
#

```
  
There are numerous NAs in certain variables, and variables with negative values. Variables with negative values have apparently normal distributions so it's possible some previous data adjustments have been made. The variable data with negative values in stable, normal distributions will be used as-is.
  
Below is a plot of the distribution of counts for the TARGET variable.  
  
```{r,echo=FALSE}
ggplot(wine, aes(TARGET)) + geom_bar(stat="count")
```
  
Here is another look at the TARGET variable, stratified by the number of Stars rating given for each wine.

```{r}
ggplot(wine, aes(TARGET, fill = STARS)) + geom_bar(stat = "count") + facet_grid(STARS ~ 
    ., margins = TRUE, scales = "free")

```
  
  
  
\begin{center}
{\huge Data Preparation}
\end{center}


##Data Preparation  
  
We will cleanse the data by removing the index column, using the MICE package to replace NA's with meaningful values, and setting the unrated wines (no stars) to zero stars, so they can be analyzed quantitatively.  
  
```{r,echo=FALSE}
# replace missing STARS values with zero's.  (like saying "Not Rated")
wine$STARS[is.na(wine$STARS)] <- 0 

# Use MICE package to fill in NA's - uses Predictive Mean Matching to fill in NA's 
wine.i <- mice(wine, m = 3, print=F)
wine <- complete(wine.i,1)# Next, get a general look at the data

f <- colnames(wine)  # establish the data categories to be studied
v <- length(f)
```

Below are boxplots of the independent variables, which illustrate the normality of the data.

  
```{r,echo=FALSE}

#generate boxplots so we can visualize the level of normality of each variable
par(mfcol=c(1,3))
for (i in 1:v){boxplot(wine[,i],main = f[i])}
```
  
  
\begin{center}
{\huge Build Models}
\end{center}


##Build Models  
  
### Regular Poisson 

To take a deeper look at the data, first we create a model for each variable individually - to get a sense of how each variable interacts with the outcome on its own, as a means to inform us how we might use groups of variables to build the best models.
  
  
```{r,echo=FALSE}

#start with generalized regression model, poisson distribution for each variable individually (baseline)
par(mfrow=c(2,3))
for (i in 2:v){
  plot(wine[,1] ~ wine[,i], xlab = f[i], ylab = f[1] )
  m <- glm(as.formula(paste(colnames(wine)[1], "~", paste(colnames(wine)[i], collapse = "+"), sep = "")),data = wine, family=poisson()) 
  abline(m,col="blue")
}
```

By looking at these models we suspect there may be two forces at work.  The first we will call Perception.  The two Perception variables are Stars and Label Appeal.  Based on the high coefficients and high significance, Perception seems to impact the outcome much more than anything else.  The second force we will call Chemistry.  All the other variables could belong to this group.  The pattern we see here is that the best outcome (highest number of cases purchased) tends to occur when the Chemistry variables are close to the mean.    

  
Next we will create a generalized linear model, Poisson family, that combines all the variables:  
  
  
```{r,echo=FALSE}
# create generalized linear model, poisson distribution.  this is for analyzing count data 
pm <- glm(as.formula(paste(colnames(wine)[1], "~", paste(colnames(wine)[-1], collapse = "+"), sep = "")),data = wine,family=poisson()) 
summary(pm)
```
  
Here we see that the Perception variables have an outsize impact on the outcome.  
  
Let's create a Poisson model using only the two Perception variables:  
  
  
```{r,echo=FALSE}

pm2 <- glm(TARGET ~ STARS + LabelAppeal,data = wine,family=poisson()) 
summary(pm2)

```
  
    
###Zero-inflated Poisson Model  
  
We next explore the seemingly high number of zero cases in the TARGET count as seen in the previous histogram. We can easily see if the number of zeros observed is in line with the number of zeros predicted by the Poisson model alone.  
  
```{r,echo=FALSE}
# build observed and predicted counts for wine cases and plot the relationship
highcnt <- max(wine$TARGET)
zerocnt <- table(wine$TARGET)[1:highcnt]
predcnt <- colSums(predprob(pm)[,1:highcnt])
plot(predcnt,zerocnt,type='n',xlab="Predicted",ylab="Observed")
text(predcnt,zerocnt,0:highcnt)

```
  
The number of observed zero cases and the predicted zero cases do not match up well so we'll move to look at the influence of the zero counts on the model by separating out the modeling of zero counts and the modeling of the non-zero counts.  

Staying with our concepts of Perception and Chemistry, we will look treating the high number of zero counts using the Perception variables of STARS and LabelAppeal, and the non-zero counts will use all other variables as the Chemistry variables.  
  
```{r,echo=FALSE}

# now build the model and run the AIC step function as usual
zp <- zeroinfl(formula=TARGET ~ . -(STARS+LabelAppeal) | STARS+LabelAppeal, data = wine, dist="poisson")
summary(zp)

# to perform a goodness test, generate a version using all of the variables
zp2 <- zeroinfl(formula=TARGET ~ ., data = wine, dist="poisson")

# prepare for a chi-squared test
(lrt <- 2 * (zp2$loglik - zp$loglik))
zp.chi <- (1-pchisq(lrt,as.integer(lrt)))
print(paste("Chi-Square Test = ",zp.chi))

```
  
Given the large p-value from the chi-square test, we conclude our model approach for Chemsitry vs Perception is valid.  
  
After analyzing the p-values for the Chemistry portion of the zero-inflated model, there are only 4 statistically significant variables: VolatileAcidity, Density, Alcohol, and AcidIndex. We'll re-reun the zero-inflated Poisson model with just these variables in the Poisson portion.
  
```{r,echo=FALSE}
zp.simplified <- zeroinfl(formula=TARGET ~ (VolatileAcidity+Density+Alcohol+AcidIndex) -(STARS+LabelAppeal) | STARS+LabelAppeal, data = wine, dist="poisson")
summary(zp.simplified)

```
  
We have reduced the degrees-of-freedom from 16 down to 8 which is as far as we'll go with the zero-inflated Poisson model.  
  
###Regular Negative Binomial Model  
  
For regular negative binomial model, we start with all the dependent variables, and perform a backward stepwise algorithm. Initially, we have 14 dependent variables; using this process we reduce to 10 variables. The AIC is 46692
  
```{r,echo=FALSE}
nb_backward <- suppressWarnings(suppressMessages(glm.nb(TARGET ~ ., data = wine)))
nb_backward <- suppressWarnings(suppressMessages(step(nb_backward, trace = 0)))
summary(nb_backward)
plot(fitted(nb_backward), residuals(nb_backward))
```
  
###Zero-inflated Negative Binomial Regession Model  
  
We'll continue our exploration of the seemingly high number of zero cases in the TARGET count as seen in the previous histogram. In this case, we'll see if the number of zeros observed is in line with the number of zeros predicted by the negative binomial model alone.  
  
```{r,echo=FALSE}
# build observed and predicted counts for wine cases and plot the relationship
highcnt <- max(wine$TARGET)
zerocnt <- table(wine$TARGET)[1:highcnt]
predcnt <- colSums(predprob(nb_backward)[,1:highcnt])
plot(predcnt,zerocnt,type='n',xlab="Predicted",ylab="Observed")
text(predcnt,zerocnt,0:highcnt)

```
  
The number of observed zero cases and the predicted zero cases do not match up well so we'll move to look at the influence of the zero counts on the model by separating out the modeling of zero counts and the modeling of the non-zero counts.  

Staying with our concepts of Perception and Chemistry, we will look at treating the high number of zero counts using the Perception variables of STARS and LabelAppeal, and the non-zero counts will use all other variables as the Chemistry variables.  
  
```{r,echo=FALSE}

# now build the model and run the AIC step function as usual
zn <- zeroinfl(formula=TARGET ~ . - (STARS+LabelAppeal) | (STARS+LabelAppeal), data = wine, dist="negbin")
summary(zn)

# to perform a goodness test, generate a version using all of the variables
zn2 <- zeroinfl(formula=TARGET ~ ., data = wine, dist="negbin")

# prepare for a chi-squared test
(lrt <- 2 * (zn2$loglik - zn$loglik))
zn.chi <- (1-pchisq(lrt,as.integer(lrt)))
print(paste("Chi-Square Test = ",zn.chi))

```
  
Given the large p-value from the chi-square test, we conclude our model approach for Chemistry versus Perception is valid.  
  
After analyzing the p-values for the Chemistry portion of the zero-inflated model, there are only four statistically significant variables: VolatileAcidity, Density, Alcohol, and AcidIndex. We'll re-reun the zero-inflated Poisson model with just these variables in the negative binomial portion.  
  
```{r,echo=FALSE}
zn.simplified <- zeroinfl(formula=TARGET ~ (VolatileAcidity+Density+Alcohol+AcidIndex) -(STARS+LabelAppeal) | STARS+LabelAppeal, data = wine, dist="negbin")
summary(zn.simplified)
```
  
We have reduced the degrees-of-freedom from 17 down to 9 which is as far as we'll go with the zero-inflated negative binomial model.  
  
\begin{center}
{\huge Select Models}
\end{center}
  
###Linear Regression Models:

Lastly we are going to look at a regular linear model, as a comparison to the analysis shown above. Again we are going to compare Chemistry vs. Perception.

```{r}
lin.mod.perc <- lm(TARGET ~ . - STARS - LabelAppeal,data = wine) 
summary(lin.mod.perc)
```


We can see that not all the chemistry variables are significant, using backward stepwise elimination, so we eliminate the insignificant independent variables.

```{r}
lin.mod.back <- step(lin.mod.perc)
```

```{r}
lin.mod.back <- lm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
    TotalSulfurDioxide + Density + pH + Sulphates + Alcohol + 
    AcidIndex, data = wine)
summary(lin.mod.back)
```
Comparing this to just the perception data we can see the following: 

```{r}
lin.mod.app <- lm(TARGET ~ STARS + LabelAppeal,data = wine) 
summary(lin.mod.app)
```

From the the models, we can clearly see that the perception data was a much more appropriate fit. This can be seen through the R-Squared value, which shows that the perception model explains roughly 50% of the variance in the model. This is a pretty "good-fit." Using the chemistry data, we also see a significant model, however, the fit is much worse, with practically none of the variance explained. 

Checking the residuals we can see the following: 


Chemistry model:
```{r}
plot(lin.mod.back)
```

Perception Model:
```{r}
plot(lin.mod.app)
```

From the plots above, we can clearly see that the first chemistry model shows clear patterns in the residuals, which indicates that linear modeling is not at all a good choice for these particular variables. However, just using the perception variables we see a much better picture, with a more random residual distribution with no clear patterns that would suggest another choice in models. 


##Select Models  
  
To compare all our regular models first, we build a dataframe which contains all the performance parameters of the models. Out of the four regular models, it is clear that the regular linear model with focus on perception is the best model. It has the lowest AIC and BIC. The Log-Likelihood is also the highest among the four. 


```{r,echo=FALSE}

m1 <- glance(pm2)
m2 <- glance(nb_backward)
m3 <- glance(lin.mod.back)
m4 <- glance(lin.mod.app)

#Compare non-zero inflated models
Models = c("Regular Poisson", "Regular Negative Binomial", "Regular Linear Science", "Regular Linear Perception")

LogLik = c(m1$logLik, m2$logLik, m3$logLik, m4$logLik)
AIC = c(m1$AIC, m2$AIC, m3$AIC, m4$AIC)
BIC = c(m1$BIC, m2$BIC, m3$BIC, m4$BIC)
Deviance = c(m1$deviance, m2$deviance, m3$deviance, m4$deviance)
df.residual = c(m1$df.residual, m2$df.residual, m3$df.residual, m4$df.residual)

any_but_zeroinflated <- data.frame(Models, AIC, BIC, LogLik, Deviance, df.residual)
any_but_zeroinflated
```
  

When we compare the two zero-inflated models against each other, the following code tells us that the performance differences between two zero inflated models (in terms of LogLik) is not statistically significant since the p value is 0.9595, which is much higher than the significance level 0.05. Their Log Likelihood are both -21962, we have to compare some other performance parameters such as AIC. zero inflated poisson model has slightly smaller AIC (43939.9) compare to the other one(43941.9)

```{r}
#Compare two zero-inflated models
lmtest::lrtest(zn.simplified, zp.simplified)
AIC(zn.simplified)
AIC(zp.simplified)
```
  
  
By comparing the regular linear model with focus on perception to the zero inflated poisson model, the histogram shows that zero inflated model takes good care of those structural zeros, which are not really zero but more like out of scope. Both models generate predictions that peak at 4 cases of wine, which correspond to the actual observation. Another thing we notice is that the predictions made by two models differ quite significantly. It is recognized according to the boxplot of the absolute differences between two models' residuals. However, based on AIC and Log Likelihood, zero inflated poisson model is still the winner here. 

```{r}
boxplot(abs(resid(lin.mod.app) - resid(zp.simplified)))

par(mfcol=c(1,3))
hist(wine$TARGET)
hist(fitted(lin.mod.app))
hist(fitted(zp.simplified))

AIC(lin.mod.app)
AIC(zp.simplified)

logLik(lin.mod.app)
logLik(zp.simplified)
```
  

```{r}
#The following code is just the data preparation step for the evaluation dataset, before we apply our model.
wine_eval <- wine_eval[2:length(wine_eval)]
wine_eval$STARS[is.na(wine_eval$STARS)] <- 0 
wine_eval <- mice(wine_eval, m = 3, print=F)
wine_eval <- complete(wine_eval,1)
```
  
###Our final predicted results.
```{r}
wine_eval$TARGET <- predict(zp.simplified, newdata = wine_eval, type = "response")
head(wine_eval, 20)
```
  
Smooth Operators - All Done!