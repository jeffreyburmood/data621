---
title: "DATA621-FinalProject-SmoothOperators"
author: "Rob Hodde, Matt Farris, Jeffrey Burmood, Bin Lin"
date: "5/26/2017"
output: pdf_document
---

\begin{center}
{\huge Introduction}
\end{center}

##Abstract 

Movies: The quintessential form of story telling that we as humans, have developed thus far. They have become the modern past-time for us, a way to escape the humdrum of everyday life into a fantasy world filled with drama, intrique and delight. Movies have astonding audience for the best part of a century, and with that, have become a vast and lucrative industry. Studios, actors and actresses, directors, and production companies make up just a small part of world of film, and we had hope that looking into some movie data we would be able to find some insight. As avid fans and lovers of all films, we decided that this project would provide us both entertainment, and revelation into a fascinated world.

##Problem Description  
  
Our final project will explore, analyze and model a data set containing information on approximately 5,000 movies. The dataset contains movie data extracted from the IMDB website and is available on Kaggle.com.

The project will develop predictive models for three questions:  
  
1) Will the movie make money or lose money?  
  
2) What is the anticipated gross margin (profit) for the movie?

3) Are there any particular genres/keywords that influence profitability?


\begin{center}
{\huge Data Exploration}
\end{center}
  
##Data Exploration

The first part of our project consists of explored our data source. As stated above, it came from Kaggle, a repository/social hub for data analyst like ourselves. The dataset isn't, by any stretch of the imagination 
  
```{r,echo=FALSE}
# Load required libraries

suppressWarnings(suppressMessages(library(ggplot2)))
suppressWarnings(suppressMessages(library(ROCR)))
suppressWarnings(suppressMessages(library(RCurl)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(Hmisc)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(stringr)))
suppressWarnings(suppressMessages(library(mice)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(reshape2)))
suppressWarnings(suppressMessages(library(MASS)))
suppressWarnings(suppressMessages(library(pscl)))
suppressWarnings(suppressMessages(library(broom)))
suppressWarnings(suppressMessages(library(lmtest)))
suppressWarnings(suppressMessages(library(moments)))
suppressWarnings(suppressMessages(library(forecast)))

# Read in the dataset from github
movies.raw <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Final/movie_metadata.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)

cpi <- read.csv(text=getURL("https://raw.githubusercontent.com/jeffreyburmood/data621/master/Final/CPI-Year.csv"),header=TRUE,na.strings=c(""," "), stringsAsFactors = FALSE)
names(cpi)<-c("title_year","cpi")

# Remove the columns not being used in the analysis
col.remove <- c(1,2,3,7,10,11,15,17,18,19,20,27,28)
movies <- subset(movies.raw,select=-col.remove)

# there are only a few NAs, go ahead and remove rows with NAs
movies <- na.omit(movies)

#Remove's Unicode Error for PDF Latex
movies$movie_title <- iconv(movies$movie_title, "latin1", "ASCII", sub="")

#Removing Foreign Films
movies <-subset(movies, country == "USA")
movies <- subset(movies, select = -country)
```
  
To this point we've removed the data columns for the variables that we will not be using in the analysis. The columns that we will focus on are the following:  
  
```{r,echo=FALSE}
names(movies)
```
  
After exploring the data, we noticed there is a scattering of NAs across the variables. Due to the relatively low number of total NAs, we choose to remove all rows with NAs, leaving 3,828 rows of data. 

Furthermore, we noticed aproximately 800 foreign films. Though we would have loved these to be apart of our data source, we realized that the budget and gross variables for these films tended to differ dramatically. We saw that the budget was usually in the currency of the country while the gross tended to be in USA dollars. Because trying to adjust for currency differences across several year, we felt it best to remove this data for simplicity sake. This left us with 3042 movies to analyse, which we felt was more than adequate for the project. 
  
Next we will explore the nature of the data for the variables we will be using in the analysis.  
  
```{r,echo=FALSE,fig.width = 8, fig.height = 3}
# Let's start by exploring the type of each variable
types <- sapply(1:length(movies),function(x) typeof(movies[,x]))
types.df <- data.frame(VAR=names(movies),TYPE=types)
kable(types.df)

# Set up content rating as factors to form a categorical variable for modeling
movies$content_rating <- factor(movies$content_rating)

# Show a statistical summary of the data
kable(summary(movies[,1:5]))
kable(summary(movies[,6:10]))
kable(summary(movies[,11:14]))
```


We also wanted to investigate the correlations, and we can see that none of the variables have any correlation that we can percieve.
```{r,echo=FALSE,fig.width = 8, fig.height = 3}
# grab just the numeric data columns for variable analysis
movies.numeric <- subset(movies,select=-c(6,10))

# start by looking for correlations between the numeric variables (ONLY the predictor variables)
movies.pred <- subset(movies.numeric, select=-c(5))
cor.table <- cor(movies.pred) # build a table of inter-variable correlation values
kable(cor.table[,1:4])
kable(cor.table[,5:8])
kable(cor.table[,9:10])
```



Lastly for exploration, we looked at all our variables through boxplots, histograms, and scatter plots. 
```{r,echo=FALSE,fig.width = 8, fig.height = 3}
# Look at the boxplots of the numeric variables
f <- colnames(movies.numeric)  # establish the data categories to be studied
par(mfrow=c(1,3))

for (i in 1:length(f)){
  boxplot(movies.numeric[,i],main = f[i])
}

# we also need to look at the histograms for the numeric variables
for (i in 1:length(f)){
  m <- mean(movies.numeric[,i])
  s <- sd(movies.numeric[,i])  
  hist(movies.numeric[,i],freq=FALSE,main = f[i],xlab="")
  curve(dnorm(x,mean=m,sd=s),col="darkblue",lwd=2,add=TRUE)
}

# let's also look at a quick plot of the data for each variable
for (i in 1:length(f)){
  plot(movies.numeric[,i],main = f[i],xlab="",ylab="")
}
```
  
As we can see from the plots and statistical summary, most of the variables have a reasonable distribution except those variable associated with the Facebook likes. There are five variables related to Facebook likes that are highly skewed due to a large number of zeros. While examining the dataset source, they revealed that along of the "zero" values from the facebook likes were caused by simple errors in the scraping. At this point we assume these zeros represent NAs in the Facebook data.  

  
Next, we'll use the mice package to impute the Facebook likes data for the zeros/NAs.
  

```{r,echo=FALSE}
# build a dataframe of just the facebook likes data
facebook.likes <- data.frame(director_facebook_likes=movies$director_facebook_likes,actor_1_facebook_likes=movies$actor_1_facebook_likes,actor_2_facebook_likes=movies$actor_2_facebook_likes,actor_3_facebook_likes=movies$actor_3_facebook_likes,cast_total_facebook_likes=movies$cast_total_facebook_likes)
# now convert all of the zero values to "NA"
facebook.likes <- sapply(facebook.likes, function(x) ifelse(x==0,NA,x))
# review the NA patterns for each variable
md.pattern(facebook.likes)
#mice PACKAGE
#uses Predictive Mean Matching. 
fb.likes.i <- mice(facebook.likes, m = 3, print=F)
facebook.likes <- complete(fb.likes.i,1)
# repopulate the movies dataframe with the imputed variable
movies$director_facebook_likes <- facebook.likes$director_facebook_likes
movies$actor_1_facebook_likes <- facebook.likes$actor_1_facebook_likes
movies$actor_2_facebook_likes <- facebook.likes$actor_2_facebook_likes
movies$actor_3_facebook_likes <- facebook.likes$actor_3_facebook_likes
movies$cast_total_facebook_likes <- facebook.likes$cast_total_facebook_likes

summary(movies)
```


\begin{center}
{\huge Data Preparation}
\end{center}


##Data Preparation  

One of the big issues faced when using this dataset is the time frame. These movies were collected over the past 80+ years, and the following shows our distribution over time: 

```{r,echo=FALSE}
hist(movies$title_year,main = "Year Released",xlab="")
```
  

As you can see, the vast majority came from 1990s and above, but we can't discredit the movies from previous year. In order to accurately portray elements from the past, we have instituted a rate of inflation calculation. Using the consumer price index (for our part here we are making a crucial assumption, that all dollars are calculated based on US currency, and we are ignoring even more complex foreign exchange rates of the time), we can calculate the gross value per year. As a basis of comparison, we are using the CPI index from 2016, as the last movie was made in 2016. 


```{r,echo=FALSE}
movies <- merge(x = movies, y = cpi, by = "title_year")
movies$adj_gross <- with(movies, (240/cpi * gross))
movies$adj_budget <- with(movies, (240/cpi * budget))
movies$adj_margin <- with(movies, adj_gross-adj_budget)
```

```{r,echo=FALSE,fig.width = 8, fig.height = 4}
attach(movies)
par(mfrow=c(2,1))
plot(title_year,gross, main="Unadjusted Gross Per Year")
plot(title_year,adj_gross,main="Adjusted Gross Per Year")
```

From the above graphs, we can see that the adjustment for the gross did indeed create a more uniformed dataset (where as before we saw movies increasing over the years). As a point of interest, the movies that made over a billion dollars are shown below:

```{r,echo=FALSE}
highest_gross <- subset(movies, adj_gross > 1000000000, select=c("movie_title", "gross", "adj_gross"))
highest_gross
```

A quick Google search indicates that the above movies are consistently listed as the top grossing movies of all time. Furthermore, our "estimated adjusted gross" mimics the findings that we see with adjusted gross (for the most part, there are two schools of thought on how to adjust gross, using ticket prices or our method adjusting based on CPI). Though our dollar amount vary slightly from other sources, any variance is consistent across our dataset, and would not negatively impact on the overall results.   

```{r,echo=FALSE}
boxplot(movies$adj_margin, main ="Profit Margin")
```


\begin{center}
{\huge Build Models}
\end{center}
##Build Models  

### Binomial Regression 

Our first model we want to investigate is whether or not we can predict if film will make money given the cast and direction. To do this, we decided to create a binary regression model, transforming our adjusted margin into a simple binary: 0 equals a loss of money, 1 equals a proft.

Below we utilized a binomial model with the logistic  regression function in R.  
  
```{r,echo=FALSE}
# Now we're ready to explore model building. In preparation, split the dataset into a training set
# and a test set
## 80% of the sample size
#Creating a binomial column 
movies_bin <- Filter(is.numeric, movies)
movies_bin$money <- 0 
movies_bin$money[movies_bin$adj_margin > 0] <-1
pred_col <- c(1,2,3,4,5,7,8,9,11,17)
movies_bin <- movies_bin[,pred_col]

set.seed(121)
smp_size <- floor(0.80 * nrow(movies_bin))
## set the seed to make your partition reproductible
train_ind <- sample(seq_len(nrow(movies_bin)), size = smp_size)
train <- movies_bin[train_ind, ]
test <- movies_bin[-train_ind, ]
```  


```{r,echo=FALSE}
#Creating the Binomial Model 
bin_movie <- glm(money ~ ., family=binomial(link='logit'),data=train)
summary(bin_movie)

```
```{r,echo=FALSE}
pred_col <- c(1,2,3,4,5,7,8,9,11)
p <- predict(bin_movie, newdata=test, type = "response")
pr <- prediction(p, test$money)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc 
```


Using all the prediction variables at hand, the model accurately predicts 74% of the time. Using backward stepwise regression, we attempted to remove some variables that may not have had significance in our model.

```{r,echo=FALSE}
backward <- step(bin_movie)
summary(backward)

p <- predict(backward, newdata=test, type="response")
pr <- prediction(p, test$money)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc_back <- performance(pr, measure = "auc")
auc_back <- auc_back@y.values[[1]]
plot(prf)
abline(a = 0, b = 1)
auc_back
```

As you can see the using backward stepwise regression produced slightly better AIC scores, however, the AUC decreased, but minimially. Another revelation, was that the Director Facebook score was not a signficant factor in our model, and was thus removed by the backward stepwise regression. It appears that for our purposes here, the actors facebook likes were better indicators of profitability that directors, which goes to show how the industry has unfolded. A few directors may have become prominent in our culture, but the recognizability of actors and actresses have a greater pull on whether or not a movie will make money. 

As a final step, we used a confusion matrix to show the relative strength of our model.

```{r}
#Creating confusion matrix
bin_prediction <- ifelse(p > 0.5, 1, 0)
confusion_bin <- confusionMatrix(data = bin_prediction, reference = test[,10])
confusion_bin$table
```

As you can see we tend to have more false negatives than false positives, and the break down of accuracy, specificity, precision and F1-score can be seen below:

```{r, echo= FALSE}
f1 <- function(s,p) {
  (2*p*s)/(p+s)
}

Parameters <- c("Accuracy", "Classification Error Rate", "Precision", "Sensitivity", "Specificity", "F1 Score")

#Creating Confusion Matrix outputs
Model1 <- c(confusion_bin$overall["Accuracy"], 1 - confusion_bin$overall["Accuracy"], confusion_bin$byClass["Pos Pred Value"], confusion_bin$byClass["Sensitivity"], confusion_bin$byClass["Specificity"], f1(confusion_bin$byClass["Sensitivity"],confusion_bin$byClass["Pos Pred Value"]))
kable(data.frame(Parameters, Model1))
```


###Profit Margin Model

We now have a model to give us an insight if a particular movie is going to make money or loss money. However, from a movie investor's perspective, that information is not sufficient to make up his or her mind to go into that business. Therefore, another focus of our final project to build a model that can predict how much money each movie makes (gross margin) or otherwise. 

Below we built multivariable linear regression model without any data transformation. 

```{r,echo=FALSE}
#Extract all numerical variables. Take care of na and excessive zeros.  
movies1 <- Filter(is.numeric, movies)
colSums(sapply(movies1, is.na))
colSums(sapply(movies1, function(x) return (x == 0)))

#Elminate title_year, gross, facenumber_in_poster(too many zeros), budget, cpi
movies1 <- subset(movies1, select = -c(1, 6, 9, 10, 13))
```


The first model we created does not show good performance. The AIC is 121904.1, BIC is 121970.4 and  logLik is -60941.07. After we take a look at the histogram of the residuals plot, we realized it is highly skewed to the right. The skewness is as high as 13.65786
```{r,echo=FALSE}
#Also exclude adj_margin when building the model, because it is simply the linear combinations of the other two variables, adj_gross and adj_budget.
m1 <- lm(adj_gross ~. - adj_margin, data = movies1) 
m1_back <- step(m1, trace = 0)

summary(m1_back)
glance(m1_back)
par(mfcol=c(2,2))
plot(m1_back)

hist(m1_back$residuals, main=NA, xlab = "m1_back Residuals", ylab = "Number of Movies")
skewness(m1_back$residuals)
```


The bad performance is most likely due to the abnormal skewness and kurtosis from the original data. Therefore, we adopted Box-Cox transformation based on normality assumption.
```{r,echo=FALSE}
duration_bc <- boxcox(movies1$duration ~ 1, plotit = FALSE)  
duration_lambda <- with(duration_bc, x[which.max(y)])
dfl_bc <- boxcox(movies1$director_facebook_likes ~ 1, plotit = FALSE)	
dfl_lambda <- with(dfl_bc, x[which.max(y)])
a3fl_bc <- boxcox(movies1$actor_3_facebook_likes ~ 1, plotit = FALSE)	
a3fl_lambda <- with(a3fl_bc, x[which.max(y)])
a1fl_bc <- boxcox(movies1$actor_1_facebook_likes ~ 1, plotit = FALSE)	
a1fl_lambda <- with(a1fl_bc, x[which.max(y)])
nvu_bc <- boxcox(movies1$num_voted_users ~ 1, plotit = FALSE)	
nvu_lambda <- with(nvu_bc, x[which.max(y)])
ctfl_bc <- boxcox(movies1$cast_total_facebook_likes ~ 1, plotit = FALSE)	
ctfl_lambda <- with(ctfl_bc, x[which.max(y)])
a2fl_bc <- boxcox(movies1$actor_2_facebook_likes ~ 1, plotit = FALSE)	
a2fl_lambda <- with(a2fl_bc, x[which.max(y)])
imdb_bc <- boxcox((movies1$imdb_score) ~ 1, plotit = FALSE)	
imdb_lambda <- with(imdb_bc, x[which.max(y)])
ab_bc <- boxcox(movies1$adj_budget ~ 1, plotit = FALSE)	
ab_lambda <- with(ab_bc, x[which.max(y)])


duration <- BoxCox(movies1$duration, duration_lambda)
director_facebook_likes <- BoxCox(movies1$director_facebook_likes, dfl_lambda)
actor_3_facebook_likes <- BoxCox(movies1$actor_3_facebook_likes, a3fl_lambda)
actor_1_facebook_likes <- BoxCox(movies1$actor_1_facebook_likes, a1fl_lambda)
num_voted_users <- BoxCox(movies1$num_voted_users, nvu_lambda)
cast_total_facebook_likes <- BoxCox(movies1$cast_total_facebook_likes, ctfl_lambda)
actor_2_facebook_likes <- BoxCox(movies1$actor_2_facebook_likes, a2fl_lambda)
imdb_score <- BoxCox(movies1$imdb_score, imdb_lambda)
adj_budget <- BoxCox(movies1$adj_budget, ab_lambda)

movies2 <- data.frame(duration, director_facebook_likes, actor_3_facebook_likes, actor_1_facebook_likes, num_voted_users, cast_total_facebook_likes, actor_2_facebook_likes, imdb_score, adj_budget)
```


The following code is just comparing distributions of the two datasets. One is dataset before transformation, then the second one is the dataset after transformation. From the comparison, we clearly notice Box-Cox transformation approximately normalize the data.

```{r,echo=FALSE}
par(mar = rep(2, 4), mfrow = c(3, 4))
for (i in 1:length(movies1))
{
    plot(density(movies1[, i]), main = colnames(movies1)[i])
}

par(mar = rep(2, 4), mfrow = c(3, 4))
for (i in 1:length(movies2))
{
    plot(density(movies2[, i]), main = colnames(movies2)[i])
}
```

Our second model has very similar AIC, BIC, and loglikelyhood values. However, the skewness of the residual histogram has been reduced. This has indicated the second model is a better model compare to the first one.

```{r,echo=FALSE}
movies2 <- cbind(movies2, movies1$adj_gross)
colnames(movies2)[10] <- "adj_gross"
m2 <- lm(adj_gross ~ ., data = movies2) 
m2_back <- step(m2, trace = 0)

summary(m2_back)
glance(m2_back)
par(mfcol=c(2,2))
plot(m2_back)

hist(m2_back$residuals, main=NA, xlab = "m2_back Residuals", ylab = "Number of Movies")
skewness(m2_back$residuals)
```


The following code is just the data preparation step for evaluation before we apply our model. Finally, we create a master dataframe containing our predicted results and actual data.
```{r,echo=FALSE}
#Add new variable profit_margin to the original movie datasets. According to investopedia, profit_margin =  net profit / revenue. For the purpose of simplisity, here we use adjusted margin to be our net profit and adjusted gross to be our revenue
movies$profit_margin <- movies$adj_margin / movies$adj_gross


#Replace the variables with the variables obtained from Box-Cox transformation
movies_copy <- movies
movies_copy$duration <- duration
movies_copy$director_facebook_likes <- director_facebook_likes
movies_copy$actor_3_facebook_likes <- actor_3_facebook_likes
movies_copy$actor_1_facebook_likes <- actor_1_facebook_likes
movies_copy$num_voted_users <- num_voted_users
movies_copy$cast_total_facebook_likes <- cast_total_facebook_likes
movies_copy$actor_2_facebook_likes <- actor_2_facebook_likes
movies_copy$imdb_score <- imdb_score
movies_copy$adj_budget <- adj_budget


#Predicted gross
gross_p <- predict(m2_back, newdata = movies_copy, type = "response")


#Calculate predicted profit margin
profit_margin_p <- (gross_p - movies$adj_budget) / gross_p


#Creating master data frame.
movies_profit <- data.frame(movies$movie_title, movies$adj_gross, gross_p, movies$profit_margin, profit_margin_p)
colnames(movies_profit) <- c("Movie Title", "Actual Adjusted Gross", "Predicted Gross", "Actual Profit Margin", "Predicted Profit Margin")
```


Final output
```{r}
head(movies_profit)
```


The predicted profit margin variable will serve as a reference for investors to decide if they want to contriubte to the production of the movies and share the profit that is generated. 

Since we have the actual profit margin variable available to us, it will also be really interesting to investigate if the quality of the movies will have any imapcts on the profitability of the movies. We will like to use IMDB rating as a standard representing movie quality.


This line of code just proves that there exists very weak positive relationship between quality and profitability of movies. The p-value is 0.007905, which is less than the significance level of 0.05. In addition, the 95% confidence interval is (0.01263244, 0.08354498), which does not cross zero. It also shows the result is statistically significant. However, the correlation coefficient is only 0.04814938, which is a very weak association between the two variable. 

Therefore, if investors care more about profitability, it is recommended not to care too much about the quality of the movie. Spending huge amount of money on improving the movie quality will lead the expected return that is minuscule.

```{r,echo=FALSE}
cor.test(movies$imdb_score, movies$profit_margin, conf.level = 0.95)
```

Smooth Operators - All Done!  
